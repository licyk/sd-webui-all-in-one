{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD Scripts Kaggle\n",
    "Created by [licyk](https://github.com/licyk)\n",
    "\n",
    "Jupyter Notebook 仓库：[licyk/sd-webui-all-in-one](https://github.com/licyk/sd-webui-all-in-one)\n",
    "\n",
    "\n",
    "## 简介\n",
    "一个在 [Kaggle](https://www.kaggle.com) 部署 [sd-scripts](https://github.com/kohya-ss/sd-scripts) 的 Jupyter Notebook，可用于 Stable Diffusion 模型的训练。\n",
    "\n",
    "\n",
    "## 不同运行单元的功能\n",
    "该 Notebook 分为以下几个单元：\n",
    "\n",
    "- [功能初始化](#功能初始化)\n",
    "- [参数配置](#参数配置)\n",
    "- [安装环境](#安装环境)\n",
    "- [模型训练](#模型训练)\n",
    "- [模型上传](#模型上传)\n",
    "\n",
    "使用时请按顺序运行笔记单元。\n",
    "\n",
    "通常情况下[功能初始化](#功能初始化)和[模型上传](#模型上传)单元的内容无需修改，其他单元包含不同功能的注释，可阅读注释获得帮助。\n",
    "\n",
    "[参数配置](#参数配置)单元用于修改安装，训练，上传模型时的配置。\n",
    "\n",
    "[安装](#安装)单元执行安装训练环境的命令和下载模型 / 训练集的命令，可根据需求进行修改。\n",
    "\n",
    "[模型训练](#模型训练)执行训练模型的命令，需要根据自己的需求进行修改，该单元也提供一些训练参数的例子，可在例子的基础上进行修改。\n",
    "\n",
    "如果需要快速取消注释，可以选中代码，按下`Ctrl + /`取消注释。\n",
    "\n",
    "\n",
    "## 提示\n",
    "1. 不同单元中包含注释, 可阅读注释获得帮助。\n",
    "2. 训练代码的部分需要根据自己的需求进行更改。\n",
    "3. 推荐使用 Kaggle 的 `Save Version` 的功能运行笔记，可让 Kaggle 笔记在无人值守下保持运行，直至所有单元运行完成。\n",
    "4. 如果有 [HuggingFace](https://huggingface.co) 账号或者 [ModelScope](https://modelscope.cn) 账号，可通过填写 Token 和仓库名后实现自动上传训练好的模型，仓库需要手动创建。\n",
    "5. 进入 Kaggle 笔记后，在 Kaggle 的右侧栏可以调整 kaggle 笔记的设置，也可以上传训练集等。注意，在 Kaggle 笔记的`Session options`->`ACCELERATOR`中，需要选择`GPU T4 x 2`，才能使用 GPU 进行模型训练。\n",
    "6. 使用 Kaggle 进行模型训练时，训练集中最好没有 NSFW 内容，否则可能会导致 Kaggle 账号被封禁。\n",
    "7. 不同单元的标题下方包含快捷跳转链接，可使用跳转链接翻阅 Notebook。\n",
    "8. 该 Notebook 的使用方法可阅读：</br>[使用 HuggingFace / ModelScope 保存和下载文件 - licyk的小窝](https://licyk.netlify.app/2025/01/16/use-huggingface-or-modelscope-to-save-file/)</br>[使用 Kaggle 进行模型训练 - licyk的小窝](https://licyk.netlify.app/2025/01/16/use-kaggle-to-training-sd-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 功能初始化\n",
    "通常不需要修改该单元的内容  \n",
    "1. [[下一个单元 →](#参数配置)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SD Scripts Manager 功能初始化部分, 通常不需要修改\n",
    "# 如果需要查看完整代码实现, 可阅读: https://github.com/licyk/sd-webui-all-in-one/blob/main/sd_scripts_ipynb_core.py\n",
    "#################################################################################################################\n",
    "# SD_SCRIPTS_IPYNB_CORE_URL, FORCE_DOWNLOAD_CORE 参数可根据需求修改, 通常保持默认即可\n",
    "SD_SCRIPTS_IPYNB_CORE_URL = \"https://github.com/licyk/sd-webui-all-in-one/raw/main/sd_scripts_ipynb_core.py\" # SD Scripts Manager 核心下载地址\n",
    "FORCE_DOWNLOAD_CORE = False # 设置为 True 时, 即使 SD Scripts Manager 已存在也会重新下载\n",
    "#################################################################################################################\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "try:\n",
    "    print(f\"[SD Scripts Manager] Jupyter Notebook 根目录: {JUPYTER_ROOT_PATH}\") # type: ignore\n",
    "except Exception as _:\n",
    "    JUPYTER_ROOT_PATH = Path(os.getcwd())\n",
    "    sys.path.append(str(JUPYTER_ROOT_PATH.resolve()))\n",
    "    print(f\"[SD Scripts Manager] Jupyter Notebook 根目录: {JUPYTER_ROOT_PATH}\")\n",
    "    SD_SCRIPTS_IPYNB_CORE_PATH = JUPYTER_ROOT_PATH / \"sd_scripts_ipynb_core.py\" # SD Scripts Manager 核心保存路径\n",
    "try:\n",
    "    if SD_SCRIPTS_IPYNB_CORE_PATH.exists() and not FORCE_DOWNLOAD_CORE:\n",
    "        print(\"[SD Scripts Manager] SD Scripts Manager 核心模块已存在\")\n",
    "    else:\n",
    "        urllib.request.urlretrieve(SD_SCRIPTS_IPYNB_CORE_URL, SD_SCRIPTS_IPYNB_CORE_PATH)\n",
    "        print(\"[SD Scripts Manager] SD Scripts Manager 核心模块下载成功\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"SD Scripts Manager 核心模块下载错误: {e}\")\n",
    "from sd_scripts_ipynb_core import logger, VERSION, SDScriptsManager\n",
    "logger.info(\"SD Scripts Manager 版本: %s\", VERSION)\n",
    "logger.info(\"核心模块初始化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数配置\n",
    "设置必要的参数, 根据注释说明进行修改  \n",
    "2. [[← 上一个单元](#功能初始化)|[下一个单元 →](#安装环境)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境设置\n",
    "WORKSPACE = \"/kaggle\" # 工作路径, 通常不需要修改\n",
    "WORKFOLDER = \"sd-scripts\" # 工作路径中文件夹名称, 通常不需要修改\n",
    "SD_SCRIPTS_REPO = \"https://github.com/kohya-ss/sd-scripts\" # sd-scripts 仓库地址\n",
    "SD_SCRIPTS_REQUIREMENT = \"requirements.txt\" # sd-scripts 依赖文件名\n",
    "TORCH_VER = \"torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124\" # PyTorch 版本\n",
    "XFORMERS_VER = \"xformers==0.0.28.post3\" # xFormers 版本\n",
    "USE_UV = True # 使用 uv 加速 Python 软件包安装, 修改为 True 为启用, False 为禁用\n",
    "PIP_INDEX_MIRROR = \"https://pypi.python.org/simple\" # PyPI 主镜像源\n",
    "PIP_EXTRA_INDEX_MIRROR = \"https://download.pytorch.org/whl/cu124\" # PyPI 扩展镜像源\n",
    "PYTORCH_MIRROR = \"https://download.pytorch.org/whl/cu124\" # 用于下载 PyTorch 的镜像源\n",
    "PIP_FIND_LINKS_MIRROR = \"https://download.pytorch.org/whl/cu121/torch_stable.html\" # PyPI 扩展镜像源\n",
    "HUGGINGFACE_MIRROR = \"https://hf-mirror.com\" # HuggingFace 镜像源\n",
    "GITHUB_MIRROR = [ # Github 镜像源\n",
    "    \"https://ghfast.top/https://github.com\",\n",
    "    \"https://mirror.ghproxy.com/https://github.com\",\n",
    "    \"https://ghproxy.net/https://github.com\",\n",
    "    \"https://gh.api.99988866.xyz/https://github.com\",\n",
    "    \"https://gh-proxy.com/https://github.com\",\n",
    "    \"https://ghps.cc/https://github.com\",\n",
    "    \"https://gh.idayer.com/https://github.com\",\n",
    "    \"https://ghproxy.1888866.xyz/github.com\",\n",
    "    \"https://slink.ltd/https://github.com\",\n",
    "    \"https://github.boki.moe/github.com\",\n",
    "    \"https://github.moeyy.xyz/https://github.com\",\n",
    "    \"https://gh-proxy.net/https://github.com\",\n",
    "    \"https://gh-proxy.ygxz.in/https://github.com\",\n",
    "    \"https://wget.la/https://github.com\",\n",
    "    \"https://kkgithub.com\",\n",
    "    \"https://gitclone.com/github.com\",\n",
    "]\n",
    "CHECK_AVALIABLE_GPU = False # 检查可用的 GPU, 当 GPU 不可用时强制终止安装进程\n",
    "RETRY = 3 # 重试下载次数\n",
    "DOWNLOAD_THREAD = 16 # 下载线程\n",
    "ENABLE_TCMALLOC = True # 启用 TCMalloc 内存优化\n",
    "ENABLE_CUDA_MALLOC = True # 启用 CUDA Malloc 显存优化\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# sd-scripts 版本设置\n",
    "SD_SCRIPTS_BRANCH = \"dev\" # sd-scripts 分支, 可切换成 main / dev 或者其它分支, 留空则不进行切换\n",
    "SD_SCRIPTS_COMMIT = \"\" # 切换 sd-scripts 的版本到某个 Git 提交记录上, 留空则不进行切换\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# 模型上传设置, 使用 HuggingFace / ModelScope 上传训练好的模型\n",
    "# HuggingFace: https://huggingface.co\n",
    "# ModelScope: https://modelscope.cn\n",
    "USE_HF_TO_SAVE_MODEL = False # 使用 HuggingFace 保存训练好的模型, 修改为 True 为启用, False 为禁用 (True / False)\n",
    "USE_MS_TO_SAVE_MODEL = False # 使用 ModelScope 保存训练好的模型, 修改为 True 为启用, False 为禁用 (True / False)\n",
    "\n",
    "# Token 配置, 用于上传 / 下载模型 (部分模型下载需要 Token 进行验证)\n",
    "# HuggingFace Token 在 Account -> Settings -> Access Tokens 中获取\n",
    "HF_TOKEN = \"\" # HuggingFace Token\n",
    "# ModelScope Token 在 首页 -> 访问令牌 -> SDK 令牌 中获取\n",
    "MS_TOKEN = \"\" # ModelScope Token\n",
    "\n",
    "# 用于上传模型的 HuggingFace 模型仓库的 ID, 当仓库不存在时则尝试新建一个\n",
    "HF_REPO_ID = \"username/reponame\" # HuggingFace 仓库的 ID (格式: \"用户名/仓库名\")\n",
    "HF_REPO_TYPE = \"model\" # HuggingFace 仓库的种类 (可选的类型为: model / dataset / space), 如果在 HuggingFace 新建的仓库为模型仓库则不需要修改\n",
    "# HuggingFace 仓库类型和对应名称:\n",
    "# model: 模型仓库\n",
    "# dataset: 数据集仓库\n",
    "# space: 在线运行空间仓库\n",
    "\n",
    "# 用于上传模型的 ModelScope 模型仓库的 ID, 当仓库不存在时则尝试新建一个\n",
    "MS_REPO_ID = \"username/reponame\" # ModelScope 仓库的 ID (格式: \"用户名/仓库名\")\n",
    "MS_REPO_TYPE = \"model\" # ModelScope 仓库的种类 (model / dataset / space), 如果在 ModelScope 新建的仓库为模型仓库则不需要修改\n",
    "# ModelScope 仓库类型和对应名称:\n",
    "# model: 模型仓库\n",
    "# dataset: 数据集仓库\n",
    "# space: 创空间仓库\n",
    "\n",
    "# 设置自动创建仓库时仓库的可见性, False 为私有仓库(不可见), True 为公有仓库(可见), 通常保持默认即可\n",
    "HF_REPO_VISIBILITY = False # 设置新建的 HuggingFace 仓库可见性 (True / False)\n",
    "MS_REPO_VISIBILITY = False # 设置新建的 ModelScope 仓库可见性 (True / False)\n",
    "\n",
    "# Git 信息设置, 可以使用默认值\n",
    "GIT_USER_EMAIL = \"username@example.com\" # Git 的邮箱\n",
    "GIT_USER_NAME = \"username\" # Git 的用户名\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# 训练日志设置, 可使用 TensorBoard / WandB 记录训练日志, 使用 WandB 可远程查看实时训练日志\n",
    "# 使用 WandB 需要填写 WANDB_TOKEN\n",
    "# 如果 TensorBoard 和 WandB 同时使用, 可以改成 all\n",
    "LOG_MODULE = \"tensorboard\" # 使用的日志记录工具 (tensorboard / wandb / all)\n",
    "\n",
    "# WandB Token 设置\n",
    "# WandB Token 可在 https://wandb.ai/authorize 中获取\n",
    "WANDB_TOKEN = \"\" # WandB Token\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# 路径设置, 通常保持默认即可\n",
    "INPUT_DATASET_PATH = \"/kaggle/dataset\" # 训练集保存的路径\n",
    "OUTPUT_PATH = \"/kaggle/working/model\" # 训练时模型保存的路径\n",
    "SD_MODEL_PATH = \"/kaggle/sd-models\" # 模型下载到的路径\n",
    "KAGGLE_INPUT_PATH = \"/kaggle/input\" # Kaggle Input 的路径\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# 训练模型设置, 在安装时将会下载选择的模型\n",
    "# 下面举个例子:\n",
    "# SD_MODEL = [\n",
    "#     [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/v1-5-pruned-emaonly.safetensors\", 0],\n",
    "#     [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/animefull-final-pruned.safetensors\", 1],\n",
    "#     [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/Counterfeit-V3.0_fp16.safetensors\", 0],\n",
    "#     [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v0.1.safetensors\", 1, \"Illustrious.safetensors\"]\n",
    "# ]\n",
    "# \n",
    "# 在这个例子中, 第一个参数指定了模型的下载链接, 第二个参数设置了是否要下载这个模型, 当这个值为 1 时则下载该模型\n",
    "# 第三个参数是可选参数, 用于指定下载到本地后的文件名称\n",
    "# \n",
    "# 则上面的例子中\n",
    "# https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/animefull-final-pruned.safetensors 和 \n",
    "# https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v0.1.safetensors 下载链接所指的文件将被下载\n",
    "# https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/animefull-final-pruned.safetensors 的文件下载到本地后名称为 animefull-final-pruned.safetensors\n",
    "# 并且 https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v0.1.safetensors 所指的文件将被重命名为 Illustrious.safetensors\n",
    "\n",
    "SD_MODEL = [\n",
    "    # Stable Diffusion 模型\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/v1-5-pruned-emaonly.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/animefull-final-pruned.safetensors\", 1],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/Counterfeit-V3.0_fp16.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/cetusMix_Whalefall2.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/cuteyukimixAdorable_neochapter3.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/ekmix-pastel-fp16-no-ema.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/ex2K_sse2.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/kohakuV5_rev2.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/meinamix_meinaV11.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/oukaStar_10.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/pastelMixStylizedAnime_pastelMixPrunedFP16.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/rabbit_v6.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/sweetSugarSyndrome_rev15.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/AnythingV5Ink_ink.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/bartstyledbBlueArchiveArtStyleFineTunedModel_v10.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/meinapastel_v6Pastel.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/qteamixQ_omegaFp16.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sd_1.5/tmndMix_tmndMixSPRAINBOW.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/sd_xl_base_1.0_0.9vae.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/sd_xl_refiner_1.0_0.9vae.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/sd_xl_turbo_1.0_fp16.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animagine-xl-3.0-base.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animagine-xl-3.0.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animagine-xl-3.1.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animagine-xl-4.0.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animagine-xl-4.0-opt.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/holodayo-xl-2.1.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kivotos-xl-2.0.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/clandestine-xl-1.0.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/UrangDiffusion-1.1.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/RaeDiffusion-XL-v2.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/sd_xl_anime_V52.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kohaku-xl-delta-rev1.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kohakuXLEpsilon_rev1.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kohaku-xl-epsilon-rev2.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kohaku-xl-epsilon-rev3.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kohaku-xl-zeta.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/starryXLV52_v52.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/heartOfAppleXL_v20.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/heartOfAppleXL_v30.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/baxlBartstylexlBlueArchiveFlatCelluloid_xlv1.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/baxlBlueArchiveFlatCelluloidStyle_xlv3.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/sanaexlAnimeV10_v10.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/sanaexlAnimeV10_v11.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/SanaeXL-Anime-v1.2-aesthetic.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/SanaeXL-Anime-v1.3-aesthetic.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v0.1.safetensors\", 1],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v0.1-GUIDED.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v1.0.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v1.1.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v2.0-stable.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/jruTheJourneyRemains_v25XL.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/PVCStyleModelMovable_illustriousxl10.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/miaomiaoHarem_v15a.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/waiNSFWIllustrious_v80.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/tIllunai3_v4.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_earlyAccessVersion.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_epsilonPred05Version.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_epsilonPred075.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_epsilonPred077.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_epsilonPred10Version.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_epsilonPred11Version.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPredTestVersion.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred05Version.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred06Version.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred065SVersion.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred075SVersion.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred09RVersion.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred10Version.safetensors\", 1],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/PVCStyleModelMovable_nbxl12.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/PVCStyleModelMovable_nbxlVPredV10.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/ponyDiffusionV6XL_v6StartWithThisOne.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/pdForAnime_v20.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/tPonynai3_v51WeightOptimized.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/omegaPonyXLAnime_v20.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animeIllustDiffusion_v061.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/artiwaifuDiffusion_v10.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/artiwaifu-diffusion-v2.safetensors\", 0],\n",
    "\n",
    "    # VAE 模型\n",
    "    [\"https://huggingface.co/licyk/sd-vae/resolve/main/sd_1.5/vae-ft-ema-560000-ema-pruned.safetensors\", 0],\n",
    "    [\"https://huggingface.co/licyk/sd-vae/resolve/main/sd_1.5/vae-ft-mse-840000-ema-pruned.safetensors\", 1],\n",
    "    [\"https://huggingface.co/licyk/sd-vae/resolve/main/sdxl_1.0/sdxl_fp16_fix_vae.safetensors\", 1],\n",
    "]\n",
    "\n",
    "##############################################################################\n",
    "# 下面为初始化参数部分, 不需要修改\n",
    "INSTALL_PARAMS = {\n",
    "    \"torch_ver\": TORCH_VER or None,\n",
    "    \"xformers_ver\": XFORMERS_VER or None,\n",
    "    \"git_branch\": SD_SCRIPTS_BRANCH or None,\n",
    "    \"git_commit\": SD_SCRIPTS_COMMIT or None,\n",
    "    \"model_path\": SD_MODEL_PATH or None,\n",
    "    \"model_list\": SD_MODEL,\n",
    "    \"use_uv\": USE_UV,\n",
    "    \"pypi_index_mirror\": PIP_INDEX_MIRROR or None,\n",
    "    \"pypi_extra_index_mirror\": PIP_EXTRA_INDEX_MIRROR or None,\n",
    "    # Kaggle 的环境暂不需要以下镜像源\n",
    "    # \"pypi_find_links_mirror\": PIP_FIND_LINKS_MIRROR or None,\n",
    "    # \"github_mirror\": GITHUB_MIRROR or None,\n",
    "    # \"huggingface_mirror\": HUGGINGFACE_MIRROR or None,\n",
    "    \"pytorch_mirror\": PYTORCH_MIRROR or None,\n",
    "    \"sd_scripts_repo\": SD_SCRIPTS_REPO or None,\n",
    "    \"sd_scripts_requirment\": SD_SCRIPTS_REQUIREMENT or None,\n",
    "    \"retry\": RETRY,\n",
    "    \"huggingface_token\": HF_TOKEN or None,\n",
    "    \"modelscope_token\": MS_TOKEN or None,\n",
    "    \"wandb_token\": WANDB_TOKEN or None,\n",
    "    \"git_username\": GIT_USER_NAME or None,\n",
    "    \"git_email\": GIT_USER_EMAIL or None,\n",
    "    \"check_avaliable_gpu\": CHECK_AVALIABLE_GPU,\n",
    "    \"enable_tcmalloc\": ENABLE_TCMALLOC,\n",
    "    \"enable_cuda_malloc\": ENABLE_CUDA_MALLOC,\n",
    "}\n",
    "os.makedirs(WORKSPACE, exist_ok=True) # 创建工作路径\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True) # 创建模型输出路径\n",
    "os.makedirs(INPUT_DATASET_PATH, exist_ok=True) # 创建训练集路径\n",
    "os.makedirs(SD_MODEL_PATH, exist_ok=True) # 创建模型下载路径\n",
    "SD_SCRIPTS_PATH = os.path.join(WORKSPACE, WORKFOLDER) # sd-scripts 路径\n",
    "logger.info(\"参数设置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装环境\n",
    "安装环境和下载模型和训练集, 根据注释的说明进行修改  \n",
    "3. [[← 上一个单元](#参数配置)|[下一个单元 →](#模型训练)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化部分参数并执行安装命令, 这一小部分不需要修改\n",
    "from tqdm import tqdm\n",
    "logger.info(\"开始安装 sd-scripts\")\n",
    "sd_scripts = SDScriptsManager(WORKSPACE, WORKFOLDER)\n",
    "sd_scripts.install(**INSTALL_PARAMS)\n",
    "\n",
    "# 将 KAGGLE_INPUT_PATH 内的文件移动到 INPUT_DATASET_PATH 指定的路径\n",
    "if os.path.exists(KAGGLE_INPUT_PATH) and len(os.listdir(KAGGLE_INPUT_PATH)) > 0:\n",
    "    logger.info(\"从 Kaggle Input 导入文件中\")\n",
    "    for i in tqdm(os.listdir(KAGGLE_INPUT_PATH), desc=\"[SD Scripts Manager] Kaggle Input 文件导入\"):\n",
    "        sd_scripts.copy_files(os.path.join(KAGGLE_INPUT_PATH, i), INPUT_DATASET_PATH)\n",
    "# 在 Kaggle 界面右侧中有 Kaggle Input 的功能, 可用于导入训练集 / 模型\n",
    "# 如果使用了 Kaggle Input 导入了训练集 / 模型, 则 KAGGLE_INPUT_PATH 中的所有文件将被复制到 INPUT_DATASET_PATH 中\n",
    "# 即将 /kaggle/input 中的所有文件复制到 /kaggle/dataset 中\n",
    "##########################################################################################\n",
    "# 下方可自行编写命令\n",
    "# 下方的命令示例可以根据自己的需求进行修改\n",
    "\n",
    "\n",
    "##### 1. 关于运行环境 #####\n",
    "\n",
    "# 如果需要安装某个软件包, 可以使用 %pip 命令\n",
    "# 下面是几个使用例子:\n",
    "# 1.\n",
    "# %pip install lycoris-lora==2.1.0.post3 dadaptation==3.1\n",
    "# \n",
    "# 这将安装 lycoris-lora==2.1.0.post3 和 dadaptation==3.1\n",
    "# \n",
    "# 2.\n",
    "# %pip uninstall tensorboard\n",
    "# \n",
    "# 这将卸载 tensorboard\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "##### 2. 关于模型导入 #####\n",
    "\n",
    "# 该 Kaggle 训练脚本支持 4 种方式导入模型, 如下:\n",
    "# 1. 使用 Kaggle Input 导入\n",
    "# 2. 使用模型下载链接导入\n",
    "# 3. 从 HuggingFace 仓库导入\n",
    "# 4. 从 ModelScope 仓库导入\n",
    "\n",
    "\n",
    "### 2.1. 使用 Kaggle Input 导入 ###\n",
    "# 在 Kaggle 右侧面板中, 点击 Notebook -> Input -> Upload -> New Model, 从此处导入模型\n",
    "\n",
    "\n",
    "### 2.2 使用模型下载链接导入 ###\n",
    "# 如果需要通过链接下载额外的模型, 可以使用 sd_scripts.get_model()\n",
    "# 使用参数:\n",
    "# sd_scripts.get_model(\n",
    "#     url=\"model_url\",                    # 模型下载链接\n",
    "#     path=SD_MODEL_PATH,                 # 模型下载到本地的路径\n",
    "#     filename=\"filename.safetensors\",    # 模型的名称\n",
    "#     retry=RETRY,                        # 重试下载的次数\n",
    "# )\n",
    "# \n",
    "# 下面是几个使用例子:\n",
    "# 1.\n",
    "# sd_scripts.get_model(\n",
    "#     url=\"https://modelscope.cn/models/user/repo/resolve/master/your_model.safetensors\",\n",
    "#     path=SD_MODEL_PATH,\n",
    "#     retry=RETRY,\n",
    "# )\n",
    "# 这将从 https://modelscope.cn/models/user/repo/resolve/master/your_model.safetensors 下载模型并保存到 SD_MODEL_PATH 中\n",
    "# \n",
    "# sd_scripts.get_model(\n",
    "#     url=\"https://modelscope.cn/models/user/repo/resolve/master/your_model.safetensors\",\n",
    "#     path=SD_MODEL_PATH,\n",
    "#     filename=\"rename_model.safetensors\",\n",
    "#     retry=RETRY,\n",
    "# )\n",
    "# 这将从 https://modelscope.cn/models/user/repo/resolve/master/your_model.safetensors 下载模型并保存到 SD_MODEL_PATH 中, 并且重命名为 rename_model.safetensors\n",
    "\n",
    "\n",
    "### 2.3. 从 HuggingFace 仓库导入 ###\n",
    "# 如果需要从 HuggingFace 仓库下载模型, 可以使用 sd_scripts.repo.download_files_from_repo()\n",
    "# 使用参数:\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"huggingface\",                 # 指定为 HuggingFace 的仓库\n",
    "#     local_dir=SD_MODEL_PATH,                # 模型下载到本地的路径\n",
    "#     repo_id=\"usename/repo_id\",              # HuggingFace 仓库 ID\n",
    "#     repo_type=\"model\",                      # (可选参数) HuggingFace 仓库种类 (model / dataset / space)\n",
    "#     folder=\"path/in/repo/file.safetensors\", # (可选参数) 文件在 HuggingFace 仓库中的路径\n",
    "#     retry=RETRY,                            # (可选参数) 重试下载的次数, 默认为 3\n",
    "#     num_threads=DOWNLOAD_THREAD,            # (可选参数) 下载线程\n",
    "# )\n",
    "# \n",
    "# 例如要从 stabilityai/stable-diffusion-xl-base-1.0 (类型为 model) 下载 sd_xl_base_1.0_0.9vae.safetensors\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"huggingface\",\n",
    "#     repo_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "#     repo_type=\"model\",\n",
    "#     folder=\"sd_xl_base_1.0_0.9vae.safetensors\",\n",
    "#     local_dir=SD_MODEL_PATH,\n",
    "#     retry=RETRY,\n",
    "#     num_threads=DOWNLOAD_THREAD,\n",
    "# )\n",
    "# 则上述的命令将会从 stabilityai/stable-diffusion-xl-base-1.0 下载 sd_xl_base_1.0_0.9vae.safetensors 模型\n",
    "# 并将模型保存到 SD_MODEL_PATH 中\n",
    "# 注意 folder 填的是文件在 HuggingFace 仓库中的路径, 如果上述例子中的文件在仓库的 checkpoint/sd_xl_base_1.0_0.9vae.safetensors 路径\n",
    "# 则 folder 填的内容为 checkpoint/sd_xl_base_1.0_0.9vae.safetensors\n",
    "#\n",
    "# 模型保存的路径与 local_dir 和 folder 参数有关\n",
    "# 对于上面的例子 local_dir 为 /kaggle/sd-models, folder 为 sd_xl_base_1.0_0.9vae.safetensors\n",
    "# 则最终保存的路径为 /kaggle/sd-models/sd_xl_base_1.0_0.9vae.safetensors\n",
    "# \n",
    "# 如果 folder 为 checkpoint/sd_xl_base_1.0_0.9vae.safetensors\n",
    "# 则最终保存的路径为 /kaggle/sd-models/checkpoint/sd_xl_base_1.0_0.9vae.safetensors\n",
    "# \n",
    "# folder 参数为可选参数, 即该参数可不指定, 在不指定的情况下将下载整个仓库中的文件\n",
    "# 比如将上面的例子改成:\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"huggingface\",\n",
    "#     repo_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "#     repo_type=\"model\",\n",
    "#     local_dir=SD_MODEL_PATH,\n",
    "#     retry=RETRY,\n",
    "#     num_threads=DOWNLOAD_THREAD,\n",
    "# )\n",
    "# 这时候将下载 stabilityai/stable-diffusion-xl-base-1.0 仓库中的所有文件\n",
    "# 对于可选参数, 可以进行省略, 此时将使用该参数的默认值进行运行, 上面的例子就可以简化成下面的:\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"huggingface\",\n",
    "#     repo_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "#     repo_type=\"model\",\n",
    "#     folder=\"sd_xl_base_1.0_0.9vae.safetensors\",\n",
    "#     local_dir=SD_MODEL_PATH,\n",
    "# )\n",
    "# 省略后仍然可以正常执行, 但对于一些重要的可选参数, 不推荐省略, 如 repo_type 参数\n",
    "# 该参数用于指定仓库类型, 不指定时则默认认为仓库为 model 类型\n",
    "# 若要下载的仓库为 dataset 类型, 不指定 repo_type 参数时默认就把仓库类型当做 model, 最终导致找不到要下载的仓库\n",
    "\n",
    "\n",
    "### 2.4. 从 ModelScope 仓库导入 ###\n",
    "# 如果需要从 ModelScope 仓库下载模型, 可以使用 sd_scripts.repo.download_files_from_repo()\n",
    "# 使用方法和 **2.3. 从 HuggingFace 仓库导入** 部分的类似, 只需要指定 api_type=\"modelscope\" 来指定使用 ModelScope 的仓库\n",
    "# 使用参数:\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"modelscope\",                  # 指定为 ModelScope 的仓库\n",
    "#     local_dir=SD_MODEL_PATH,                # 模型下载到本地的路径\n",
    "#     repo_id=\"usename/repo_id\",              # ModelScope 仓库 ID\n",
    "#     repo_type=\"model\",                      # (可选参数) ModelScope 仓库种类 (model / dataset / space)\n",
    "#     folder=\"path/in/repo/file.safetensors\", # (可选参数) 文件在 ModelScope 仓库中的路径\n",
    "#     retry=RETRY,                            # (可选参数) 重试下载的次数, 默认为 3\n",
    "#     num_threads=DOWNLOAD_THREAD,            # (可选参数) 下载线程\n",
    "# )\n",
    "# \n",
    "# 例如要从 stabilityai/stable-diffusion-xl-base-1.0 (类型为 model) 下载 sd_xl_base_1.0_0.9vae.safetensors\n",
    "# sd_scripts.dataset.get_single_file_from_ms(\n",
    "#     api_type=\"modelscope\",\n",
    "#     repo_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "#     repo_type=\"model\",\n",
    "#     folder=\"sd_xl_base_1.0_0.9vae.safetensors\",\n",
    "#     local_dir=SD_MODEL_PATH,\n",
    "#     retry=RETRY,\n",
    "#     num_threads=DOWNLOAD_THREAD,\n",
    "# )\n",
    "# 则上述的命令将会从 stabilityai/stable-diffusion-xl-base-1.0 下载 sd_xl_base_1.0_0.9vae.safetensors 模型\n",
    "# 并将模型保存到 SD_MODEL_PATH 中\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "##### 3. 关于训练集导入 #####\n",
    "\n",
    "# 该 Kaggle 训练脚本支持 4 种方式导入训练集, 如下:\n",
    "# 1. 使用 Kaggle Input 导入\n",
    "# 2. 使用训练集下载链接导入\n",
    "# 3. 从 HuggingFace 仓库导入\n",
    "# 4. 从 ModelScope 仓库导入\n",
    "\n",
    "\n",
    "### 3.1. 使用 Kaggle Input 导入 ###\n",
    "# 在 Kaggle 右侧面板中, 点击 Notebook -> Input -> Upload -> New Dataset, 从此处导入模型\n",
    "\n",
    "\n",
    "### 3.2. 使用训练集下载链接导入 ###\n",
    "# 如果将训练集压缩后保存在某个平台, 如 HuggingFace, ModelScope, 并且有下载链接\n",
    "# 可以使用 sd_scripts.utils.download_archive_and_unpack() 函数下载训练集\n",
    "# 使用参数:\n",
    "# sd_scripts.utils.download_archive_and_unpack(\n",
    "#     url=\"download_url\",             # 训练集压缩包的下载链接\n",
    "#     local_dir=INPUT_DATASET_PATH,   # 下载数据集到本地的路径\n",
    "#     name=\"filename.zip\",            # (可选参数) 将数据集压缩包进行重命名\n",
    "#     retry=RETRY,                    # (可选参数) 重试下载的次数\n",
    "# )\n",
    "# \n",
    "# 该函数在下载训练集压缩包完成后将解压到指定的本地路径\n",
    "# 压缩包格式仅支持 7z, zip, tar\n",
    "# \n",
    "# 下面是几个使用的例子:\n",
    "# 1.\n",
    "# sd_scripts.utils.download_archive_and_unpack(\n",
    "#     url=\"https://modelscope.cn/models/user/repo/resolve/master/data_1.7z\",\n",
    "#     local_dir=INPUT_DATASET_PATH,\n",
    "#     retry=RETRY,\n",
    "# )\n",
    "# 这将从 https://modelscope.cn/models/user/repo/resolve/master/data_1.7z 下载训练集压缩包并解压到 INPUT_DATASET_PATH 中\n",
    "# \n",
    "# 2.\n",
    "# sd_scripts.utils.download_archive_and_unpack(\n",
    "#     url=\"https://modelscope.cn/models/user/repo/resolve/master/data_1.7z\",\n",
    "#     local_dir=INPUT_DATASET_PATH,\n",
    "#     name=\"training_dataset.7z\",\n",
    "#     retry=RETRY,\n",
    "# )\n",
    "# 这将从 https://modelscope.cn/models/user/repo/resolve/master/data_1.7z 下载训练集压缩包并重命名成 training_dataset.7z\n",
    "# 再将 training_dataset.7z 中的文件解压到 INPUT_DATASET_PATH 中\n",
    "# \n",
    "# \n",
    "# 训练集的要求:\n",
    "# 需要将图片进行打标, 并调整训练集为指定的目结构, 例如:\n",
    "# Nachoneko\n",
    "#     └── 1_nachoneko\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.png\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.txt\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).png\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).txt\n",
    "#             ├── 0(8).txt\n",
    "#             ├── 0(8).webp\n",
    "#             ├── 001_2.png\n",
    "#             ├── 001_2.txt\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.png\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.txt\n",
    "#             ├── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.png\n",
    "#             └── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.txt\n",
    "# \n",
    "# 在 Nachoneko 文件夹新建一个文件夹, 格式为 <数字>_<名称>, 如 1_nachoneko, 前面的数字代表这部分的训练集的重复次数, 1_nachoneko 文件夹内则放图片和打标文件\n",
    "# \n",
    "# 训练集也可以分成多个部分组成, 例如:\n",
    "# Nachoneko\n",
    "#     ├── 1_nachoneko\n",
    "#     │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.png\n",
    "#     │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.txt\n",
    "#     │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).png\n",
    "#     │       └── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).txt\n",
    "#     ├── 2_nachoneko\n",
    "#     │       ├── 0(8).txt\n",
    "#     │       ├── 0(8).webp\n",
    "#     │       ├── 001_2.png\n",
    "#     │       └── 001_2.txt\n",
    "#     └── 4_nachoneko\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.png\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.txt\n",
    "#             ├── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.png\n",
    "#             └── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.txt\n",
    "# \n",
    "# 处理好训练集并调整好目录结构后可以将 Nachoneko 文件夹进行压缩了, 使用 zip / 7z / tar 格式进行压缩\n",
    "# 例如将上述的训练集压缩成 Nachoneko.7z, 此时需要检查一下压缩后在压缩包的目录结果是否和原来的一致(有些压缩软件在部分情况下会破坏原来的目录结构)\n",
    "# 确认没有问题后将该训练集上传到网盘, 推荐使用 HuggingFace / ModelScope\n",
    "\n",
    "\n",
    "### 3.3. 从 HuggingFace 仓库导入 ###\n",
    "# 如果训练集保存在 HuggingFace, 可以使用 sd_scripts.repo.download_files_from_repo() 函数从 HuggingFace 下载数据集\n",
    "# 使用方法和 **2.3. 从 HuggingFace 仓库导入** 部分类似, 部分说明可参考那部分的内容\n",
    "# 使用格式:\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"huggingface\",                 # 指定为 HuggingFace 的仓库\n",
    "#     local_dir=INPUT_DATASET_PATH,           # 下载数据集到哪个路径\n",
    "#     repo_id=\"username/train_data\",          # HuggingFace 仓库 ID\n",
    "#     repo_type=\"dataset\",                    # (可选参数) HuggingFace 仓库的类型 (model / dataset / space)\n",
    "#     folder=\"folder_in_repo\",                # (可选参数) 指定要从 HuggingFace 仓库里下载哪个文件夹的内容\n",
    "#     retry=RETRY,                            # (可选参数) 重试下载的次数, 默认为 3\n",
    "#     num_threads=DOWNLOAD_THREAD,            # (可选参数) 下载线程\n",
    "# )\n",
    "# \n",
    "# 比如在 HuggingFace 的仓库为 username/train_data, 仓库类型为 dataset\n",
    "# 仓库的文件结构如下:\n",
    "# ├── Nachoneko\n",
    "# │   ├── 1_nachoneko\n",
    "# │   │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.png\n",
    "# │   │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.txt\n",
    "# │   │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).png\n",
    "# │   │       └── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).txt\n",
    "# │   ├── 2_nachoneko\n",
    "# │   │       ├── 0(8).txt\n",
    "# │   │       ├── 0(8).webp\n",
    "# │   │       ├── 001_2.png\n",
    "# │   │       └── 001_2.txt\n",
    "# │   └── 4_nachoneko\n",
    "# │           ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.png\n",
    "# │           ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.txt\n",
    "# │           ├── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.png\n",
    "# │           └── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.txt\n",
    "# └ aaaki\n",
    "#   ├── 1_aaaki\n",
    "#   │   ├── 1.png\n",
    "#   │   ├── 1.txt\n",
    "#   │   ├── 11.png\n",
    "#   │   ├── 11.txt\n",
    "#   │   ├── 12.png\n",
    "#   │   └── 12.txt\n",
    "#   └── 3_aaaki\n",
    "#       ├── 14.png\n",
    "#       ├── 14.txt\n",
    "#       ├── 16.png\n",
    "#       └── 16.txt\n",
    "#\n",
    "# 此时想要下载这个仓库中的 Nachoneko 文件夹的内容, 则下载命令为\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"huggingface\",\n",
    "#     local_dir=INPUT_DATASET_PATH,\n",
    "#     repo_id=\"username/train_data\",\n",
    "#     repo_type=\"dataset\",\n",
    "#     folder=\"Nachoneko\",\n",
    "#     retry=RETRY,\n",
    "#     num_threads=DOWNLOAD_THREAD,\n",
    "# )\n",
    "# \n",
    "# 如果想下载整个仓库, 则移除 folder 参数, 命令修改为\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"huggingface\",\n",
    "#     local_dir=INPUT_DATASET_PATH,\n",
    "#     repo_id=\"username/train_data\",\n",
    "#     repo_type=\"dataset\",\n",
    "#     retry=RETRY,\n",
    "#     num_threads=DOWNLOAD_THREAD,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# 4. 从 ModelScope 仓库导入\n",
    "# 如果训练集保存在 ModelScope, 可以使用 sd_scripts.repo.download_files_from_repo() 函数从 ModelScope 下载数据集\n",
    "# 使用方法可参考 **3.2. 使用训练集下载链接导入** 部分的说明\n",
    "# 使用格式:\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"modelscope\",          # 指定为 ModelScope 的仓库\n",
    "#     local_dir=INPUT_DATASET_PATH,   # 下载数据集到哪个路径\n",
    "#     repo_id=\"usename/repo_id\",      # ModelScope 仓库 ID\n",
    "#     repo_type=\"dataset\",            # (可选参数) ModelScope 仓库的类型 (model / dataset / space)\n",
    "#     folder=\"folder_in_repo\",        # (可选参数) 指定要从 ModelScope 仓库里下载哪个文件夹的内容\n",
    "#     retry=RETRY,                    # (可选参数) 重试下载的次数, 默认为 3\n",
    "#     num_threads=DOWNLOAD_THREAD,    # (可选参数) 下载线程\n",
    "# )\n",
    "# \n",
    "# 比如在 ModelScope 的仓库为 username/train_data, 仓库类型为 dataset\n",
    "# 仓库的文件结构如下:\n",
    "# ├── Nachoneko\n",
    "# │   ├── 1_nachoneko\n",
    "# │   │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.png\n",
    "# │   │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.txt\n",
    "# │   │       ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).png\n",
    "# │   │       └── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).txt\n",
    "# │   ├── 2_nachoneko\n",
    "# │   │       ├── 0(8).txt\n",
    "# │   │       ├── 0(8).webp\n",
    "# │   │       ├── 001_2.png\n",
    "# │   │       └── 001_2.txt\n",
    "# │   └── 4_nachoneko\n",
    "# │           ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.png\n",
    "# │           ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.txt\n",
    "# │           ├── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.png\n",
    "# │           └── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.txt\n",
    "# └ aaaki\n",
    "#   ├── 1_aaaki\n",
    "#   │   ├── 1.png\n",
    "#   │   ├── 1.txt\n",
    "#   │   ├── 11.png\n",
    "#   │   ├── 11.txt\n",
    "#   │   ├── 12.png\n",
    "#   │   └── 12.txt\n",
    "#   └── 3_aaaki\n",
    "#       ├── 14.png\n",
    "#       ├── 14.txt\n",
    "#       ├── 16.png\n",
    "#       └── 16.txt\n",
    "#\n",
    "# 此时想要下载这个仓库中的 Nachoneko 文件夹的内容, 则下载命令为\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"modelscope\",\n",
    "#     local_dir=INPUT_DATASET_PATH,\n",
    "#     repo_id=\"username/train_data\",\n",
    "#     repo_type=\"dataset\",\n",
    "#     folder=\"Nachoneko\",\n",
    "# )\n",
    "# \n",
    "# 如果想下载整个仓库, 则移除 folder 参数, 命令修改为\n",
    "# sd_scripts.repo.download_files_from_repo(\n",
    "#     api_type=\"modelscope\",\n",
    "#     local_dir=INPUT_DATASET_PATH,\n",
    "#     repo_id=\"username/train_data\",\n",
    "#     repo_type=\"dataset\",\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# 下载训练集的技巧\n",
    "# 如果有个 character_aaaki 训练集上传到 HuggingFace 上时结构如下：\n",
    "# \n",
    "#\n",
    "# HuggingFace_Repo (licyk/sd_training_dataset)\n",
    "# ├── character_aaaki\n",
    "# │   ├── 1_aaaki\n",
    "# │   │   ├── 1.png\n",
    "# │   │   ├── 1.txt\n",
    "# │   │   ├── 3.png\n",
    "# │   │   └── 3.txt\n",
    "# │   └── 2_aaaki\n",
    "# │       ├── 4.png\n",
    "# │       └── 4.txt\n",
    "# ├── character_robin\n",
    "# │   └── 1_xxx\n",
    "# │       ├── 11.png\n",
    "# │       └── 11.txt\n",
    "# └── style_pvc\n",
    "#     └── 5_aaa\n",
    "#         ├── test.png\n",
    "#         └── test.txt\n",
    "#\n",
    "# \n",
    "# 可能有时候不想为训练集中每个子训练集设置不同的重复次数，又不想上传的时候再多套一层文件夹，就把训练集结构调整成了下面的：\n",
    "# \n",
    "#\n",
    "# HuggingFace_Repo (licyk/sd_training_dataset)\n",
    "# ├── character_aaaki\n",
    "# │   ├── 1.png\n",
    "# │   ├── 1.txt\n",
    "# │   ├── 3.png\n",
    "# │   ├── 3.txt\n",
    "# │   ├── 4.png\n",
    "# │   └── 4.txt\n",
    "# ├── character_robin\n",
    "# │   └── 1_xxx\n",
    "# │       ├── 11.png\n",
    "# │       └── 11.txt\n",
    "# └── style_pvc\n",
    "#     └── 5_aaa\n",
    "#         ├── test.png\n",
    "#         └── test.txt\n",
    "#\n",
    "# \n",
    "# 此时这个状态的训练集是缺少子训练集和重复次数的，如果直接使用 sd_scripts.repo.download_files_from_repo() 去下载训练集并用于训练将会导致报错\n",
    "# 不过可以自己再编写一个函数对 sd_scripts.repo.download_files_from_repo() 函数再次封装，自动加上子训练集并设置重复次数\n",
    "# \n",
    "#\n",
    "# def make_dataset(\n",
    "#     local_dir: str | Path,\n",
    "#     repo_id: str,\n",
    "#     repo_type: str,\n",
    "#     repeat: int,\n",
    "#     folder: str,\n",
    "# ) -> None:\n",
    "#     import os\n",
    "#     import shutil\n",
    "#     origin_dataset_path = os.path.join(local_dir, folder)\n",
    "#     tmp_dataset_path = os.path.join(local_dir, f\"{repeat}_{folder}\")\n",
    "#     new_dataset_path = os.path.join(origin_dataset_path, f\"{repeat}_{folder}\")\n",
    "#     sd_scripts.repo.download_files_from_repo(\n",
    "#         api_type=\"huggingface\",\n",
    "#         local_dir=local_dir,\n",
    "#         repo_id=repo_id,\n",
    "#         repo_type=repo_type,\n",
    "#         folder=folder,\n",
    "#     )\n",
    "#     if os.path.exists(origin_dataset_path):\n",
    "#         logger.info(\"设置 %s 训练集的重复次数为 %s\", folder, repeat)\n",
    "#         shutil.move(origin_dataset_path, tmp_dataset_path)\n",
    "#         shutil.move(tmp_dataset_path, new_dataset_path)\n",
    "#     else:\n",
    "#         logger.error(\"从 %s 下载 %s 失败\", repo_id, folder)\n",
    "#\n",
    "# \n",
    "# 编写好后，可以去调用这个函数\n",
    "# \n",
    "#\n",
    "# make_dataset(\n",
    "#     local_dir=INPUT_DATASET_PATH,\n",
    "#     repo_id=\"licyk/sd_training_dataset\",\n",
    "#     repo_type=\"dataset\",\n",
    "#     repeat=3,\n",
    "#     folder=\"character_aaaki\",\n",
    "# )\n",
    "#\n",
    "# \n",
    "# 该函数将会把 character_aaaki 训练集下载到 {INPUT_DATASET_PATH} 中，即 /kaggle/dataset\n",
    "# 文件夹名称为 character_aaaki，并且 character_aaaki 文件夹内继续创建了一个子文件夹作为子训练集，根据 repeat=3 将子训练集的重复次数设置为 3\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "logger.info(\"sd-scripts 安装完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "需自行编写命令，下方有可参考的例子  \n",
    "4. [[← 上一个单元](#安装环境)|[下一个单元 →](#模型上传)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"进入 sd-scripts 目录\")\n",
    "os.chdir(SD_SCRIPTS_PATH)\n",
    "print(\"=\" * 50)\n",
    "logger.info(\"训练集目录中的文件列表\")\n",
    "if os.path.exists(INPUT_DATASET_PATH):\n",
    "    print(f\"训练集路径: {INPUT_DATASET_PATH}\")\n",
    "    for i in os.listdir(INPUT_DATASET_PATH):\n",
    "        print(f\":: {i}\")\n",
    "print(\"=\" * 50)\n",
    "logger.info(\"模型目录中的文件列表\")\n",
    "if os.path.exists(SD_MODEL_PATH):\n",
    "    print(f\"模型路径: {SD_MODEL_PATH}\")\n",
    "    for i in os.listdir(SD_MODEL_PATH):\n",
    "        print(f\":: {i}\")\n",
    "print(\"=\" * 50)\n",
    "logger.info(\"使用 sd-scripts 进行模型训练\")\n",
    "##########################################################################################\n",
    "# 1.\n",
    "# 运行前需要根据自己的需求更改参数\n",
    "# \n",
    "# 训练参数的设置可参考：\n",
    "# https://rentry.org/59xed3\n",
    "# https://github.com/kohya-ss/sd-scripts?tab=readme-ov-file#links-to-usage-documentation\n",
    "# https://github.com/bmaltais/kohya_ss/wiki/LoRA-training-parameters\n",
    "# https://sd-moadel-doc.maozi.io\n",
    "# \n",
    "# \n",
    "# 2.\n",
    "# 下方被注释的代码选择后使用 Ctrl + / 取消注释\n",
    "# \n",
    "# \n",
    "# 3.\n",
    "# 训练使用的底模会被下载到 SD_MODEL_PATH, 即 /kaggle/sd-models\n",
    "# 填写底模路径时一般可以通过 --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/base_model.safetensors\" 指定\n",
    "# 如果需要外挂 VAE 模型可以通过 --vae=\"{SD_MODEL_PATH}/vae.safetensors\" 指定\n",
    "# \n",
    "# 通过 Kaggle Inout 导入的训练集保存在 KAGGLE_INPUT_PATH, 即 /kaggle/input, 运行该笔记时将会把训练集复制进 INPUT_DATASET_PATH, 即 /kaggle/dataset\n",
    "# 该路径可通过 INPUT_DATASET_PATH 调整\n",
    "# 如果使用 sd_scripts.dataset.get_dataset() 函数下载训练集, 数据集一般会解压到 INPUT_DATASET_PATH, 这取决于函数第一个参数传入的路径\n",
    "# 训练集的路径通常要这种结构\n",
    "# $ tree /kaggle\n",
    "# kaggle\n",
    "# └── dataset\n",
    "#     └── Nachoneko\n",
    "#         └── 1_gan_cheng\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.png\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2019 winter 麗.txt\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).png\n",
    "#             ├── [メロンブックス (よろず)]Melonbooks Girls Collection 2020 spring 彩 (オリジナル).txt\n",
    "#             ├── 0(8).txt\n",
    "#             ├── 0(8).webp\n",
    "#             ├── 001_2.png\n",
    "#             ├── 001_2.txt\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.png\n",
    "#             ├── 0b1c8893-c9aa-49e5-8769-f90c4b6866f5.txt\n",
    "#             ├── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.png\n",
    "#             └── 0d5149dd-3bc1-484f-8c1e-a1b94bab3be5.txt\n",
    "# 4 directories, 12 files\n",
    "# 在填写训练集路径时, 应使用 --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\"\n",
    "# \n",
    "# 模型保存的路径通常用 --output_dir=\"{OUTPUT_PATH}\" 指定, 如 --output_dir=\"{OUTPUT_PATH}/Nachoneko\", OUTPUT_PATH 默认设置为 /kaggle/working/model\n",
    "# 在 Kaggle 的 Output 中可以看到保存的模型, 前提是使用 Kaggle 的 Save Version 运行 Kaggle\n",
    "# OUTPUT_PATH 也指定了保存模型到 HuggingFace / ModelScope 的功能的上传路径\n",
    "# \n",
    "# --output_name 用于指定保存的模型名字, 如 --output_name=\"Nachoneko\"\n",
    "# \n",
    "# \n",
    "# 4.\n",
    "# Kaggle 的实例最长可运行 12 h, 要注意训练时长不要超过 12 h, 否则将导致训练被意外中断, 并且最后的模型保存功能将不会得到运行\n",
    "# 如果需要在模型被保存后立即上传到 HuggingFace 进行保存, 可使用启动参数为 sd-scripts 设置自动保存, 具体可阅读 sd-scripts 的帮助信息\n",
    "# 使用 python train_network.py -h 命令可查询可使用的启动参数, 命令中的 train_network.py 可替换成 sdxl_train_network.py 等\n",
    "# \n",
    "# \n",
    "# 5.\n",
    "# 训练命令的开头为英文的感叹号, 也就是 !, 后面就是 Shell Script 风格的命令\n",
    "# 每行的最后为反斜杠用于换行, 也就是用 \\ 来换行, 并且反斜杠的后面不允许有其他符号, 比如空格等\n",
    "# 训练命令的每一行之间不能有任何换行空出来, 最后一行不需要反斜杠, 因为最后一行的下一行已经没有训练参数\n",
    "# \n",
    "# \n",
    "# 6.\n",
    "# 如果训练参数是 toml 格式的, 比如从 Akegarasu/lora-scripts 训练器复制来的训练参数\n",
    "# 可以转换成对应的训练命令中的参数\n",
    "# 下面列举几种转换例子:\n",
    "# \n",
    "# (1)\n",
    "# toml 格式:\n",
    "# pretrained_model_name_or_path = \"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\"\n",
    "# 训练命令格式:\n",
    "# --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\"\n",
    "# \n",
    "# (2)\n",
    "# toml 格式:\n",
    "# unet_lr = 0.0001\n",
    "# 训练命令格式:\n",
    "# --unet_lr=0.0001\n",
    "# \n",
    "# (3)\n",
    "# toml 格式:\n",
    "# network_args = [\n",
    "#     \"conv_dim=100000\",\n",
    "#     \"conv_alpha=100000\",\n",
    "#     \"algo=lokr\",\n",
    "#     \"dropout=0\",\n",
    "#     \"factor=8\",\n",
    "#     \"train_norm=True\",\n",
    "#     \"preset=full\",\n",
    "# ]\n",
    "# 训练命令格式:\n",
    "# --network_args \\\n",
    "#     conv_dim=100000 \\\n",
    "#     conv_alpha=100000 \\\n",
    "#     algo=lokr \\\n",
    "#     dropout=0 \\\n",
    "#     factor=8 \\\n",
    "#     train_norm=True \\\n",
    "#     preset=full \\\n",
    "# \n",
    "# (4)\n",
    "# toml 格式:\n",
    "# enable_bucket = true\n",
    "# 训练命令格式:\n",
    "# --enable_bucket\n",
    "# \n",
    "# (5)\n",
    "# toml 格式:\n",
    "# lowram = false\n",
    "# 训练命令格式:\n",
    "# 无对应的训练命令, 也就是不需要填, 因为这个参数的值为 false, 也就是无对应的参数, 如果值为 true, 则对应训练命令中的 --lowram\n",
    "# \n",
    "# 可以根据这个例子去转换 toml 格式的训练参数成训练命令的格式\n",
    "# \n",
    "# \n",
    "# 7.\n",
    "# 如果需要 toml 格式的配置文件来配置训练参数可以使用下面的代码来保存 toml 格式的训练参数\n",
    "# \n",
    "# toml_file_path = os.path.join(WORKSPACE, \"train_config.toml\")\n",
    "# toml_content = f\"\"\"\n",
    "# 这里使用 toml 格式编写训练参数, \n",
    "# 还可以结合 Python F-Strings 的用法使用前面配置好的变量\n",
    "# Python F-Strings 的说明: https://docs.python.org/zh-cn/3.13/reference/lexical_analysis.html#f-strings\n",
    "# toml 的语法可参考: https://toml.io/cn/v1.0.0\n",
    "# 下面展示训练命令里参数对应的 toml 格式转换\n",
    "# \n",
    "# \n",
    "# pretrained_model_name_or_path = \"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\"\n",
    "# 对应训练命令中的 --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\"\n",
    "# \n",
    "# unet_lr = 0.0001\n",
    "# 对应训练命令中的 --unet_lr=0.0001\n",
    "# \n",
    "# network_args = [\n",
    "#     \"conv_dim=100000\",\n",
    "#     \"conv_alpha=100000\",\n",
    "#     \"algo=lokr\",\n",
    "#     \"dropout=0\",\n",
    "#     \"factor=8\",\n",
    "#     \"train_norm=True\",\n",
    "#     \"preset=full\",\n",
    "# ]\n",
    "# 对应下面训练命令中的\n",
    "# --network_args \\\n",
    "#     conv_dim=100000 \\\n",
    "#     conv_alpha=100000 \\\n",
    "#     algo=lokr \\\n",
    "#     dropout=0 \\\n",
    "#     factor=8 \\\n",
    "#     train_norm=True \\\n",
    "#     preset=full \\\n",
    "# \n",
    "# enable_bucket = true\n",
    "# 对应训练命令中的 --enable_bucket\n",
    "# \n",
    "# lowram = false\n",
    "# 这个参数的值为 false, 也就是无对应的参数, 如果值为 true, 则对应训练命令中的 --lowram\n",
    "# \"\"\".strip()\n",
    "# if not os.path.exists(os.path.dirname(toml_file_path)):\n",
    "#     os.makedirs(toml_file_path, exist_ok=True)\n",
    "# with open(toml_file_path, \"w\", encoding=\"utf8\") as file:\n",
    "#     file.write(toml_content)\n",
    "# \n",
    "# 使用上面的代码将会把训练参数的 toml 配置文件保存在 toml_file_path 路径中, 也就是 {WORKSPACE}/train_config.toml, 即 /kaggle/train_config.toml\n",
    "# 而原来的训练命令无需再写上训练参数, 只需指定该训练配置文件的路径即可\n",
    "# 使用 --config_file=\"{WORKSPACE}/train_config.toml\" 来指定\n",
    "# \n",
    "# \n",
    "# 8. \n",
    "# 如果要查看 sd-script 的命令行参数, 可以加上 -h 后再运行, 此时 sd-script 将显示所有可用的参数\n",
    "# \n",
    "# \n",
    "# 9.\n",
    "# 下方提供了一些训练参数, 可以直接使用, 使用时取消注释后根据需求修改部分参数即可\n",
    "# \n",
    "#              .,@@@@@@@@@].                                          ./@[`....`[\\\\.                 \n",
    "#             //\\`..  . ...,\\@].       .,]]]/O@@@@@@@@\\]...       .,]//............\\@`               \n",
    "#           .O`........ .......\\\\.]]@@@@@@@@[..........,[@@@@\\`.*/....=^............/@@`             \n",
    "#          .O........    .......@@/@@@/`.....               . ,\\@\\....\\`............O@`@             \n",
    "#          =^...`....          .O@@`.........            .........\\@`...[`.,@`....,@^/.@^            \n",
    "#         .OO`..\\....          =/..... ......            ..[@]....,\\@@]]]].@@]`..//..@=\\^            \n",
    "#          @O/@`,............=O/......    ...   ....       ...\\\\.....,@@@`=\\@\\@@[...=O`/^.           \n",
    "#          @@\\.,@]..]]//[,/@^O=@.............   .\\@^...........,@`.....\\@@/*\\o*O@\\.=/.@`             \n",
    "#          ,@/O`...[OOO`.,@O,\\/....././\\^....   ..@O` ..\\`.......=\\.....=\\\\@@@@@/\\@@//               \n",
    "#            ,@`\\].......O^o,/.....@`/=^.....,\\...,@^ ...=\\...    =\\.....,@,@@@@[/@@@/               \n",
    "#            ..,\\@\\]]]]O/@.*@.....=^/\\^......=@....\\O..^..@@`..  ..\\@.....,@.\\@@\\[[O`                \n",
    "#                .*=@@\\@@^.O^...../o^O.......O=^...=@..@..\\.\\\\.   . @@`....,@.\\@@@@`                 \n",
    "# .              ..=@O^=@`,@ .....@@=`......=^.O....@..@^.=^.=@.....=@@.....,\\.\\@@@@.                \n",
    "#                .,@@`,O@./^.....=@O/......./^.O... \\`.=^.=^..=@...  O=\\.....=^.\\@@@@`.              \n",
    "#                ./@`.=^@=@......=@O`....@,/@@.=^...=^.=^.=^.[[\\@....=^\\^.....@@.\\@@@@`              \n",
    "#               .,@^. @^O/@......=@O.]O`OO.=`\\^.....,^.=@.=^....=@...=\\.O.....@^\\`@@/`               \n",
    "#                =@ .=@..@^ .....=@/.../@^,/.=@......* =@.=^.....=\\..=@`=^....=^ \\/\\                 \n",
    "#                /^..=@.,@^ /`...=@.../O@.O...@........O=^=` ,`...@^.=@\\=^..] =@..@O`.               \n",
    "#               ,@...@/.=@. @^...=@../@\\@/OOO.=^......=^,O@[.]]@\\]/@ =^@`O..O.=@^ =@^                \n",
    "#               =^...@@.=@..O\\....@ //.O@O@@]..@....../^.OO@@@[[@@@@\\/^@^O .O.=@@ .@^                \n",
    "#               @^..=@@.,@.=^@....@@\\@@@[[[[[[[\\^@^..,/..O..,@@@\\..=@@//OO..O./^@. =@                \n",
    "#               @...=^@^.@.=^=^...=@@`/@@@@@`...*O\\..@...[.=@`,@@@`.@`=^@=`.O.@.@. =@.               \n",
    "#               @^..O.@^ \\^=@,\\..=@@ @\\,@@/@@`..=^..@`.....@@\\@@/@@...O.@=^,O=^.@^./@                \n",
    "#               @@..O.=@.,\\=@^O..=`\\/@^/@@OO@^..,`,O`.. .. @@/@@\\@@..=`=@../^O..@^/O^                \n",
    "#              .=@^ @..@`=@o@@=^..,.O@@/@@oO@........... ...@^.\\/@..=^=@/ =@O. .@\\@`.                \n",
    "#               .@@.@^.@^@^\\@@O^..=^,O@@*.................  .......=^/@@^=@@^ .=@`.                  \n",
    "#               .=O@O^,@^=^.\\@^o..=/\\,\\ .....   .... .....    ...]@O`=@O/@@^   =@                    \n",
    "#                .=O@O/^==@@`O@O^.=@.\\`\\`....      .  ........ ......//@.@`.   =^                    \n",
    "#                  ,O@@^.O..\\@@@^.=@...[O@`..      .  ........ .....//.@@,\\   .@^                    \n",
    "#                   .@/@@@]../@@^..@@\\........     ..,/`=@/@`.....,@^..=^ =` .=@.                    \n",
    "#                   =/..O... @^=\\. O@@@@\\....... ...//.,@..@O].,/@`=\\..=@..@..@^                     \n",
    "#                  ,/..O....=/=/@ .=@@@@@@@@].....//.,O@`.//.@@@@@..@^..@`.=\\/O`.                    \n",
    "#                 ,@../`....O=/.@...@@@@@@@@@@@@@/../\\@..//.=@@@@\\. @@..=\\..@^/.                     \n",
    "#                ,@`.=`....=@/..@...\\@@@@@@@@.. O..,@/..=@ .O=@@@^. @/@..@^..@`.                     \n",
    "#               ,@`.=^....,@/..=O^..,@@.[@@@@,@]@../^..=@../^=@@@...@.,\\.=@`..@`                     \n",
    "#              ,@..=/.   .@/...O=@...@@....,@@...,[@\\.,@`..@..@@/..,@..,\\.@@...@..                   \n",
    "#              @`.,/.....@/...=`O@^..,@...../`.......,\\@]..@..O@\\]]/\\`..,\\,@^..,@`                   \n",
    "#             =^..@...../@...,^/O@@...=@`...@............\\@` /`,\\,@`.=\\.,\\@=@`..,@.                  \n",
    "#            ,@../`....@\\`/@``,`]/@^...O,\\/\\@]..............\\\\..=@`\\\\ ,@@@@@@@....@`.                \n",
    "#            O`.=^....@^O@^.@@@@@@@\\....\\@^=@@@@@\\] ..........,@`\\@\\.@`=@@@@@@\\....@`..              \n",
    "#           =/..O...,@O/@^.@@@@@@@@@`...=/.@@@@@@@@@@@].........,@@/@`,@/@@@@O\\^....@`..             \n",
    "#           /^.O.../@O^@^./@@@@@@@@@\\...@`=@@@@@@@@@@@@@\\.......@`=\\//@`,@@@@@.@`....\\`...           \n",
    "#         .,@.=^../`@@\\@.=@@@@@@@@@@@^.=@.@@@@@@@@@@@@@@@@@`...@` =\\\\==`@`\\@@@^.@.....\\^...          \n",
    "#       ../\\^,@.,/.=@/=/,@@@@@@@@@@@@\\ @^=@@@@@@@@@@@@@@@@@@@]@@@@@@@@@o\\@`.@@\\.,@.....\\\\...         \n",
    "#       =/.@^@`/`..@@^@^=@@@@@@@@@@@@@\\@.@@@@@@@@@@@@@@@@@@@@@@@@O@@@@@\\/.,/.@@\\.,@.....,@`..        \n",
    "#     ,O`.,@=@/...=@@.@^O@@@@@@@@@@@@@@^=@@@@@@@@@@@@@@@@@@@@@@@@@@O@@@^ /@@.,@/@`.@`.....\\\\...      \n",
    "#  ..=^...=^@^....@OO,@^/@@@@@@@@@@@@\\@.@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\@@@@@`=\\.\\^.\\`.....,@`..    \n",
    "# \n",
    "# 炼丹就不存在什么万能参数的, 可能下面给的参数里有的参数训练出来的效果很好, 有的效果就一般\n",
    "# 训练参数还看训练集呢, 同一套参数在不同训练集上效果都不一样, 可能好可能坏 (唉, 被这训练参数折磨过好多次)\n",
    "# 虽然说一直用同个效果不错的参数可能不会出现特别坏的结果吧\n",
    "# 还有好的训练集远比好的训练参数更重要\n",
    "# 好的训练集真的, 真的, 真的非常重要\n",
    "# 再好的参数, 训练集烂也救不回来\n",
    "# \n",
    "# \n",
    "# 10.\n",
    "# 建议先改改训练集路径的参数就开始训练, 跑通训练了再试着改其他参数\n",
    "# 还有我编写的训练参数不一定是最好的, 所以需要自己去摸索这些训练参数是什么作用的, 再去修改\n",
    "# 其实有些参数我自己也调不明白, 但是很多时候跑出来效果还不错\n",
    "# 为什么效果好, 分からない, 这东西像个黑盒, 有时候就觉得神奇呢\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数在 animagine-xl-3.1.safetensors 测试, 大概在 30 ~ 40 Epoch 有比较好的效果 (在 36 Epoch 出好效果的概率比较高)\n",
    "#\n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/animagine-xl-3.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=50 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.0001 \\\n",
    "#     --unet_lr=0.0001 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --lr_scheduler_num_cycles=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数是在 Illustrious-XL-v0.1.safetensors 模型上测出来的, 大概在 32 Epoch 左右有比较好的效果\n",
    "# 用 animagine-xl-3.1.safetensors 那套参数也有不错的效果, 只是学习率比这套低了点, 学得慢一点\n",
    "#\n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00012 \\\n",
    "#     --unet_lr=0.00012 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --lr_scheduler_num_cycles=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数是在 Illustrious-XL-v0.1.safetensors 模型上测出来的, 大概在 32 Epoch 左右有比较好的效果\n",
    "# 用 animagine-xl-3.1.safetensors 那套参数也有不错的效果, 只是学习率比这套低了点, 学得慢一点\n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00012 \\\n",
    "#     --unet_lr=0.00012 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数是在 Illustrious-XL-v0.1.safetensors 模型上测出来的, 大概在 32 Epoch 左右有比较好的效果\n",
    "# 用 animagine-xl-3.1.safetensors 那套参数也有不错的效果, 只是学习率比这套低了点, 学得慢一点\n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# 在 --network_args 设置了 preset，可以调整训练网络的大小\n",
    "# 该值默认为 full，而使用 attn-mlp 可以得到更小的 LoRA 但几乎不影响 LoRA 效果\n",
    "# 可用的预设可阅读文档: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Preset.md\n",
    "# 该预设也可以自行编写并指定, 编写例子可查看: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/example_configs/preset_configs/example.toml\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00012 \\\n",
    "#     --unet_lr=0.00012 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#         preset=\"attn-mlp\" \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数是在 Illustrious-XL-v0.1.safetensors 模型上测出来的, 大概在 32 Epoch 左右有比较好的效果\n",
    "# 用 animagine-xl-3.1.safetensors 那套参数也有不错的效果, 只是学习率比这套低了点, 学得慢一点\n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# 在 --network_args 设置了 preset，可以调整训练网络的大小\n",
    "# 该值默认为 full，而使用 attn-mlp 可以得到更小的 LoRA 但几乎不影响 LoRA 效果\n",
    "# 可用的预设可阅读文档: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Preset.md\n",
    "# 该预设也可以自行编写并指定, 编写例子可查看: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/example_configs/preset_configs/example.toml\n",
    "# \n",
    "# 使用 --optimizer_args 设置 weight_decay 和 betas, 更高的 weight_decay 可以降低拟合程度, 减少过拟合\n",
    "# 如果拟合程度不够高, 可以提高 --max_train_epochs 的值, 或者适当降低 weight_decay 的值, 可自行测试\n",
    "# 较小的训练集适合使用较小的值, 如 0.05, 较大的训练集适合用 0.1\n",
    "# 当 weight_decay 设置为 0.05 时, 大概在 38 Epoch 有比较好的效果\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00012 \\\n",
    "#     --unet_lr=0.00012 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#         preset=\"attn-mlp\" \\\n",
    "#     --optimizer_args \\\n",
    "#         weight_decay=0.1 \\\n",
    "#         betas=\"0.9,0.95\" \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# (自己在用的)\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "# \n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# 在 --network_args 设置了 preset, 可以调整训练网络的大小\n",
    "# 该值默认为 full, 如果使用 attn-mlp 可以得到更小的 LoRA, 但对于难学的概念使用 full 效果会更好\n",
    "# \n",
    "# 可用的预设可阅读文档: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Preset.md\n",
    "# 该预设也可以自行编写并指定, 编写例子可查看: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/example_configs/preset_configs/example.toml\n",
    "# \n",
    "# 使用 --optimizer_args 设置 weight_decay 和 betas, 更高的 weight_decay 可以降低拟合程度, 减少过拟合\n",
    "# 如果拟合程度不够高, 可以提高 --max_train_epochs 的值, 或者适当降低 weight_decay 的值, 可自行测试\n",
    "# 较小的训练集适合使用较小的值, 如 0.05, 较大的训练集适合用 0.1\n",
    "# 大概 34 Epoch 会有比较好的效果吧, 不过不好说, 看训练集\n",
    "# 自己测的时候大概在 26~40 Epoch 之间会出现好结果, 测试了很多炉基本都在这个区间里, 但也不排除意外情况 (训练参数这东西好麻烦啊, 苦い)\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.0001 \\\n",
    "#     --unet_lr=0.0001 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#         preset=\"full\" \\\n",
    "#     --optimizer_args \\\n",
    "#         weight_decay=0.05 \\\n",
    "#         betas=\"0.9,0.95\" \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# (自己在用的)\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "# \n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# 在 --network_args 设置了 preset, 可以调整训练网络的大小\n",
    "# 该值默认为 full, 如果使用 attn-mlp 可以得到更小的 LoRA, 但对于难学的概念使用 full 效果会更好 (最好还是 full 吧, 其他的预设效果不是很好)\n",
    "# \n",
    "# 可用的预设可阅读文档: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Preset.md\n",
    "# 该预设也可以自行编写并指定, 编写例子可查看: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/example_configs/preset_configs/example.toml\n",
    "# \n",
    "# 使用 --optimizer_args 设置 weight_decay 和 betas, 更高的 weight_decay 可以降低拟合程度, 减少过拟合\n",
    "# 如果拟合程度不够高, 可以提高 --max_train_epochs 的值, 或者适当降低 weight_decay 的值, 可自行测试\n",
    "# 较小的训练集适合使用较小的值, 如 0.05, 较大的训练集适合用 0.1\n",
    "# 大概 34 Epoch 会有比较好的效果吧, 不过不好说, 看训练集\n",
    "# 自己测的时候大概在 26~40 Epoch 之间会出现好结果, 测试了很多炉基本都在这个区间里, 但也不排除意外情况 (训练参数这东西好麻烦啊, 苦い)\n",
    "# \n",
    "# 测试的时候发现 --debiased_estimation_loss 对于训练效果的有些改善\n",
    "# 这里有个对比: https://licyk.netlify.app/2025/02/10/debiased_estimation_loss_in_stable_diffusion_model_training\n",
    "# 启用后能提高拟合速度和颜色表现吧, 画风的学习能学得更好\n",
    "# 但, 肢体崩坏率可能会有点提高, 不过有另一套参数去优化了一下这个问题, 貌似会好一点\n",
    "# 可能画风会弱化, 所以不是很确定哪个比较好用, 只能自己试了\n",
    "# debiased estimation loss 有个相关的论文可以看看: https://arxiv.org/abs/2310.08442\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.0001 \\\n",
    "#     --unet_lr=0.0001 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#         preset=\"full\" \\\n",
    "#     --optimizer_args \\\n",
    "#         weight_decay=0.05 \\\n",
    "#         betas=\"0.9,0.95\" \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --debiased_estimation_loss \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# (自己在用的)\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "# \n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# 在 --network_args 设置了 preset, 可以调整训练网络的大小\n",
    "# 该值默认为 full, 如果使用 attn-mlp 可以得到更小的 LoRA, 但对于难学的概念使用 full 效果会更好 (最好还是 full 吧, 其他的预设效果不是很好)\n",
    "# \n",
    "# 可用的预设可阅读文档: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Preset.md\n",
    "# 该预设也可以自行编写并指定, 编写例子可查看: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/example_configs/preset_configs/example.toml\n",
    "# \n",
    "# 使用 --optimizer_args 设置 weight_decay 和 betas, 更高的 weight_decay 可以降低拟合程度, 减少过拟合\n",
    "# 如果拟合程度不够高, 可以提高 --max_train_epochs 的值, 或者适当降低 weight_decay 的值, 可自行测试\n",
    "# 较小的训练集适合使用较小的值, 如 0.05, 较大的训练集适合用 0.1\n",
    "# 大概 34 Epoch 会有比较好的效果吧, 不过不好说, 看训练集\n",
    "# 自己测的时候大概在 26~40 Epoch 之间会出现好结果, 测试了很多炉基本都在这个区间里, 但也不排除意外情况 (训练参数这东西好麻烦啊, 苦い)\n",
    "# \n",
    "# 测试的时候发现 --debiased_estimation_loss 对于训练效果的有些改善\n",
    "# 这里有个对比: https://licyk.netlify.app/2025/02/10/debiased_estimation_loss_in_stable_diffusion_model_training\n",
    "# 启用后能提高拟合速度和颜色表现吧, 画风的学习能学得更好\n",
    "# 但, 肢体崩坏率可能会有点提高, 不过有另一套参数去优化了一下这个问题, 貌似会好一点\n",
    "# 可能画风会弱化, 所以不是很确定哪个比较好用, 只能自己试了\n",
    "# debiased estimation loss 有个相关的论文可以看看: https://arxiv.org/abs/2310.08442\n",
    "# \n",
    "# 加上 v 预测参数进行训练, 提高模型对暗处和亮处的表现效果, 并且能让模型能够直出纯黑色背景, 画面也更干净\n",
    "# 相关的论文可以看看: https://arxiv.org/abs/2305.08891\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/noobaiXLNAIXL_vPred10Version.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.0001 \\\n",
    "#     --unet_lr=0.0001 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#         preset=\"full\" \\\n",
    "#     --optimizer_args \\\n",
    "#         weight_decay=0.05 \\\n",
    "#         betas=\"0.9,0.95\" \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --debiased_estimation_loss \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --zero_terminal_snr \\\n",
    "#     --v_parameterization \\\n",
    "#     --scale_v_pred_loss_like_noise_pred \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "# \n",
    "# 在 --network_args 设置了 preset, 可以调整训练网络的大小\n",
    "# 该值默认为 full, 如果使用 attn-mlp 可以得到更小的 LoRA, 但对于难学的概念使用 full 效果会更好 (最好还是 full 吧, 其他的预设效果不是很好)\n",
    "# \n",
    "# 可用的预设可阅读文档: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Preset.md\n",
    "# 该预设也可以自行编写并指定, 编写例子可查看: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/example_configs/preset_configs/example.toml\n",
    "# \n",
    "# 使用 --optimizer_args 设置 weight_decay 和 betas, 更高的 weight_decay 可以降低拟合程度, 减少过拟合\n",
    "# 如果拟合程度不够高, 可以提高 --max_train_epochs 的值, 或者适当降低 weight_decay 的值, 可自行测试\n",
    "# 较小的训练集适合使用较小的值, 如 0.05, 较大的训练集适合用 0.1\n",
    "# 大概 34 Epoch 会有比较好的效果吧, 不过不好说, 看训练集\n",
    "# 自己测的时候大概在 26~40 Epoch 之间会出现好结果, 测试了很多炉基本都在这个区间里, 但也不排除意外情况 (训练参数这东西好麻烦啊, 苦い)\n",
    "# \n",
    "# 测试的时候发现 --debiased_estimation_loss 对于训练效果的有些改善\n",
    "# 这里有个对比: https://licyk.netlify.app/2025/02/10/debiased_estimation_loss_in_stable_diffusion_model_training\n",
    "# 启用后能提高拟合速度和颜色表现吧, 画风的学习能学得更好\n",
    "# 把学习率调度器 constant_with_warmup 换成了cosine, 稍微缓解了一下拟合速度过快导致肢体崩坏率增大的问题\n",
    "# 如果学的效果不够好, 拟合度不够高, 可以适当增加 --max_train_epochs 的值或者提高训练集的重复次数\n",
    "# debiased estimation loss 有个相关的论文可以看看: https://arxiv.org/abs/2310.08442\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.0001 \\\n",
    "#     --unet_lr=0.0001 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"cosine\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#         preset=\"full\" \\\n",
    "#     --optimizer_args \\\n",
    "#         weight_decay=0.05 \\\n",
    "#         betas=\"0.9,0.95\" \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --debiased_estimation_loss \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 人物 LoRA, 使用多卡进行训练\n",
    "# 适合极少图或者单图训练集进行人物 LoRA 训练\n",
    "# 训练集使用打标器进行打标后, 要保留的人物的哪些特征, 就把对应的 Tag 删去, 触发词可加可不加\n",
    "# \n",
    "# 该参数使用 scale_weight_norms 降低过拟合程度, 进行训练时, 可在控制台输出看到 Average key norm 这个值\n",
    "# 通常测试 LoRA 时就测试 Average key norm 值在 0.5 ~ 0.9 之间的保存的 LoRA 模型\n",
    "# max_train_epochs 设置为 200, save_every_n_epochs 设置为 1 以为了更好的挑选最好的结果\n",
    "# \n",
    "# 可使用该方法训练一个人物 LoRA 模型用于生成人物的图片, 并将这些图片重新制作成训练集\n",
    "# 再使用不带 scale_weight_norms 的训练参数进行训练, 通过这种方式, 可以在图片极少的情况下得到比较好的 LoRA 模型\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=200 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00012 \\\n",
    "#     --unet_lr=0.00012 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --scale_weight_norms=1 \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用 rtx 4060 8g laptop 进行训练, 通过 fp8 降低显存占用\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数是在 Illustrious-XL-v0.1.safetensors 模型上测出来的, 大概在 32 Epoch 左右有比较好的效果\n",
    "# 用 animagine-xl-3.1.safetensors 那套参数也有不错的效果, 只是学习率比这套低了点, 学得慢一点\n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# !python \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=3 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.0002 \\\n",
    "#     --unet_lr=0.0002 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"AdamW8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --fp8_base\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "#\n",
    "# 这个参数是在 Illustrious-XL-v0.1.safetensors 模型上测出来的, 大概在 32 Epoch 左右有比较好的效果\n",
    "# 用 animagine-xl-3.1.safetensors 那套参数也有不错的效果, 只是学习率比这套低了点, 学得慢一点\n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# 参数加上了 noise_offset, 可以提高暗处和亮处的表现, 一般使用设置成 0.05 ~ 0.1\n",
    "# 但 noise_offset 可能会导致画面泛白, 光影效果变差\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/Nachoneko\" \\\n",
    "#     --output_name=\"Nachoneko_2\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/Nachoneko\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00012 \\\n",
    "#     --unet_lr=0.00012 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --noise_offset=0.1 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 人物 LoRA, 使用多卡进行训练\n",
    "# 参数中使用了 --scale_weight_norms, 用于提高泛化性, 但可能会造成拟合度降低\n",
    "# 如果当训练人物 LoRA 的图片较多时, 可考虑删去该参数\n",
    "# 当训练人物 LoRA 的图片较少, 为了避免过拟合, 就可以考虑使用 --scale_weight_norms 降低过拟合概率\n",
    "#\n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/animagine-xl-3.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/robin\" \\\n",
    "#     --output_name=\"robin_1\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/robin_1\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=50 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --learning_rate=0.0001 \\\n",
    "#     --unet_lr=0.0001 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --lr_scheduler_num_cycles=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --scale_weight_norms=1 \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 人物 LoRA, 使用多卡进行训练\n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/animagine-xl-3.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/murasame_(senren)_3\" \\\n",
    "#     --output_name=\"murasame_(senren)_10\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/murasame_(senren)_10\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=50 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --learning_rate=0.0001 \\\n",
    "#     --unet_lr=0.0001 \\\n",
    "#     --text_encoder_lr=0.00004 \\\n",
    "#     --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --lr_scheduler_num_cycles=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --scale_weight_norms=1 \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用单卡进行训练 (Kaggle 的单 Tesla P100 性能不如双 Tesla T4, 建议使用双卡训练)\n",
    "# !python \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/animagine-xl-3.1.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/rafa\" \\\n",
    "#     --output_name=\"rafa_1\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/rafa\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"1024,1024\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=4096 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=50 \\\n",
    "#     --train_batch_size=6 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00007 \\\n",
    "#     --unet_lr=0.00007 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "#     --lr_warmup_steps=0 \\\n",
    "#     --lr_scheduler_num_cycles=1 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 SD1.5 画风 LoRA, 使用双卡进行训练\n",
    "# 使用 NovelAI 1 模型进行训练\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/animefull-final-pruned.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/vae-ft-mse-840000-ema-pruned.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/sunfish\" \\\n",
    "#     --output_name=\"nai1-sunfish_5\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/nai1-sunfish_5\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"768,768\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=1024 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=12 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --network_train_unet_only \\\n",
    "#     --learning_rate=0.00024 \\\n",
    "#     --unet_lr=0.00024 \\\n",
    "#     --text_encoder_lr=0.00001 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "# 使用 lokr 算法训练 SD1.5 多画风(多概念) LoRA, 使用双卡进行训练\n",
    "# 使用 NovelAI 1 模型进行训练\n",
    "# \n",
    "# 在 SD1.5 中训练 Text Encoder 可以帮助模型更好的区分不同的画风(概念)\n",
    "# \n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/train_network.py\" \\\n",
    "#     --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/animefull-final-pruned.safetensors\" \\\n",
    "#     --vae=\"{SD_MODEL_PATH}/vae-ft-mse-840000-ema-pruned.safetensors\" \\\n",
    "#     --train_data_dir=\"{INPUT_DATASET_PATH}/sunfish\" \\\n",
    "#     --output_name=\"nai1-sunfish_5\" \\\n",
    "#     --output_dir=\"{OUTPUT_PATH}/nai1-sunfish_5\" \\\n",
    "#     --wandb_run_name=\"Nachoneko\" \\\n",
    "#     --log_tracker_name=\"lora-Nachoneko\" \\\n",
    "#     --prior_loss_weight=1 \\\n",
    "#     --resolution=\"768,768\" \\\n",
    "#     --enable_bucket \\\n",
    "#     --min_bucket_reso=256 \\\n",
    "#     --max_bucket_reso=1024 \\\n",
    "#     --bucket_reso_steps=64 \\\n",
    "#     --save_model_as=\"safetensors\" \\\n",
    "#     --save_precision=\"fp16\" \\\n",
    "#     --save_every_n_epochs=1 \\\n",
    "#     --max_train_epochs=40 \\\n",
    "#     --train_batch_size=12 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --learning_rate=0.00028 \\\n",
    "#     --unet_lr=0.00028 \\\n",
    "#     --text_encoder_lr=0.000015 \\\n",
    "#     --lr_scheduler=\"constant_with_warmup\" \\\n",
    "#     --lr_warmup_steps=100 \\\n",
    "#     --optimizer_type=\"Lion8bit\" \\\n",
    "#     --network_module=\"lycoris.kohya\" \\\n",
    "#     --network_dim=100000 \\\n",
    "#     --network_alpha=100000 \\\n",
    "#     --network_args \\\n",
    "#         conv_dim=100000 \\\n",
    "#         conv_alpha=100000 \\\n",
    "#         algo=lokr \\\n",
    "#         dropout=0 \\\n",
    "#         factor=8 \\\n",
    "#         train_norm=True \\\n",
    "#     --log_with=\"{LOG_MODULE}\" \\\n",
    "#     --logging_dir=\"{OUTPUT_PATH}/logs\" \\\n",
    "#     --caption_extension=\".txt\" \\\n",
    "#     --shuffle_caption \\\n",
    "#     --keep_tokens=0 \\\n",
    "#     --max_token_length=225 \\\n",
    "#     --seed=1337 \\\n",
    "#     --mixed_precision=\"fp16\" \\\n",
    "#     --xformers \\\n",
    "#     --cache_latents \\\n",
    "#     --cache_latents_to_disk \\\n",
    "#     --persistent_data_loader_workers \\\n",
    "#     --vae_batch_size=4 \\\n",
    "#     --full_fp16\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# 下面是 toml 格式的训练命令, 根据上面的训练命令做了格式转换\n",
    "# 只弄了自己常用的训练参数, 其他的参照下面的例子来改吧\n",
    "# \n",
    "# toml 转换格式如下 (在最前面已经写过一次了, 再写一遍方便对照):\n",
    "# (1)\n",
    "# toml 格式:\n",
    "# pretrained_model_name_or_path = \"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\"\n",
    "# 训练命令格式:\n",
    "# --pretrained_model_name_or_path=\"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\"\n",
    "# \n",
    "# (2)\n",
    "# toml 格式:\n",
    "# unet_lr = 0.0001\n",
    "# 训练命令格式:\n",
    "# --unet_lr=0.0001\n",
    "# \n",
    "# (3)\n",
    "# toml 格式:\n",
    "# network_args = [\n",
    "#     \"conv_dim=100000\",\n",
    "#     \"conv_alpha=100000\",\n",
    "#     \"algo=lokr\",\n",
    "#     \"dropout=0\",\n",
    "#     \"factor=8\",\n",
    "#     \"train_norm=True\",\n",
    "#     \"preset=full\",\n",
    "# ]\n",
    "# 训练命令格式:\n",
    "# --network_args \\\n",
    "#     conv_dim=100000 \\\n",
    "#     conv_alpha=100000 \\\n",
    "#     algo=lokr \\\n",
    "#     dropout=0 \\\n",
    "#     factor=8 \\\n",
    "#     train_norm=True \\\n",
    "#     preset=full \\\n",
    "# \n",
    "# (4)\n",
    "# toml 格式:\n",
    "# enable_bucket = true\n",
    "# 训练命令格式:\n",
    "# --enable_bucket\n",
    "# \n",
    "# (5)\n",
    "# toml 格式:\n",
    "# lowram = false\n",
    "# 训练命令格式:\n",
    "# 无对应的训练命令, 也就是不需要填, 因为这个参数的值为 false, 也就是无对应的参数, 如果值为 true, 则对应训练命令中的 --lowram\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# (自己在用的, toml 格式的版本)\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "# \n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# 在 --network_args 设置了 preset, 可以调整训练网络的大小\n",
    "# 该值默认为 full, 如果使用 attn-mlp 可以得到更小的 LoRA, 但对于难学的概念使用 full 效果会更好\n",
    "# \n",
    "# 可用的预设可阅读文档: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Preset.md\n",
    "# 该预设也可以自行编写并指定, 编写例子可查看: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/example_configs/preset_configs/example.toml\n",
    "# \n",
    "# 使用 --optimizer_args 设置 weight_decay 和 betas, 更高的 weight_decay 可以降低拟合程度, 减少过拟合\n",
    "# 如果拟合程度不够高, 可以提高 --max_train_epochs 的值, 或者适当降低 weight_decay 的值, 可自行测试\n",
    "# 较小的训练集适合使用较小的值, 如 0.05, 较大的训练集适合用 0.1\n",
    "# 大概 34 Epoch 会有比较好的效果吧, 不过不好说, 看训练集\n",
    "# 自己测的时候大概在 26~40 Epoch 之间会出现好结果, 测试了很多炉基本都在这个区间里, 但也不排除意外情况 (训练参数这东西好麻烦啊, 苦い)\n",
    "# \n",
    "# toml_file_path = os.path.join(WORKSPACE, \"train_config.toml\")\n",
    "# toml_content = f\"\"\"\n",
    "# pretrained_model_name_or_path = \"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\"\n",
    "# vae = \"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\"\n",
    "# train_data_dir = \"{INPUT_DATASET_PATH}/Nachoneko\"\n",
    "# output_name = \"Nachoneko_2\"\n",
    "# output_dir = \"{OUTPUT_PATH}/Nachoneko\"\n",
    "# wandb_run_name = \"Nachoneko\"\n",
    "# log_tracker_name = \"lora-Nachoneko\"\n",
    "# prior_loss_weight = 1\n",
    "# resolution = \"1024,1024\"\n",
    "# enable_bucket = true\n",
    "# min_bucket_reso = 256\n",
    "# max_bucket_reso = 4096\n",
    "# bucket_reso_steps = 64\n",
    "# save_model_as = \"safetensors\"\n",
    "# save_precision = \"fp16\"\n",
    "# save_every_n_epochs = 1\n",
    "# max_train_epochs = 40\n",
    "# train_batch_size = 6\n",
    "# gradient_checkpointing = true\n",
    "# network_train_unet_only = true\n",
    "# learning_rate = 0.0001\n",
    "# unet_lr = 0.0001\n",
    "# text_encoder_lr = 0.00001\n",
    "# lr_scheduler = \"constant_with_warmup\"\n",
    "# lr_warmup_steps = 100\n",
    "# optimizer_type = \"Lion8bit\"\n",
    "# network_module = \"lycoris.kohya\"\n",
    "# network_dim = 100000\n",
    "# network_alpha = 100000\n",
    "# network_args = [\n",
    "#     \"conv_dim=100000\",\n",
    "#     \"conv_alpha=100000\",\n",
    "#     \"algo=lokr\",\n",
    "#     \"dropout=0\",\n",
    "#     \"factor=8\",\n",
    "#     \"train_norm=True\",\n",
    "#     \"preset=full\",\n",
    "# ]\n",
    "# optimizer_args = [\n",
    "#     \"weight_decay=0.05\",\n",
    "#     \"betas=0.9,0.95\",\n",
    "# ]\n",
    "# log_with = \"{LOG_MODULE}\"\n",
    "# logging_dir = \"{OUTPUT_PATH}/logs\"\n",
    "# caption_extension = \".txt\"\n",
    "# shuffle_caption = true\n",
    "# keep_tokens = 0\n",
    "# max_token_length = 225\n",
    "# seed = 1337\n",
    "# mixed_precision = \"fp16\"\n",
    "# xformers = true\n",
    "# cache_latents = true\n",
    "# cache_latents_to_disk = true\n",
    "# persistent_data_loader_workers = true\n",
    "# vae_batch_size = 4\n",
    "# full_fp16 = true\n",
    "# \"\"\".strip()\n",
    "# if not os.path.exists(os.path.dirname(toml_file_path)):\n",
    "#     os.makedirs(toml_file_path, exist_ok=True)\n",
    "# with open(toml_file_path, \"w\", encoding=\"utf8\") as file:\n",
    "#     file.write(toml_content)\n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --config_file=\"{toml_file_path}\"\n",
    "\n",
    "\n",
    "# (自己在用的, toml 格式的版本)\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "# \n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# 在 --network_args 设置了 preset, 可以调整训练网络的大小\n",
    "# 该值默认为 full, 如果使用 attn-mlp 可以得到更小的 LoRA, 但对于难学的概念使用 full 效果会更好 (最好还是 full 吧, 其他的预设效果不是很好)\n",
    "# \n",
    "# 可用的预设可阅读文档: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Preset.md\n",
    "# 该预设也可以自行编写并指定, 编写例子可查看: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/example_configs/preset_configs/example.toml\n",
    "# \n",
    "# 使用 --optimizer_args 设置 weight_decay 和 betas, 更高的 weight_decay 可以降低拟合程度, 减少过拟合\n",
    "# 如果拟合程度不够高, 可以提高 --max_train_epochs 的值, 或者适当降低 weight_decay 的值, 可自行测试\n",
    "# 较小的训练集适合使用较小的值, 如 0.05, 较大的训练集适合用 0.1\n",
    "# 大概 34 Epoch 会有比较好的效果吧, 不过不好说, 看训练集\n",
    "# 自己测的时候大概在 26~40 Epoch 之间会出现好结果, 测试了很多炉基本都在这个区间里, 但也不排除意外情况 (训练参数这东西好麻烦啊, 苦い)\n",
    "# \n",
    "# 测试的时候发现 --debiased_estimation_loss 对于训练效果的有些改善\n",
    "# 这里有个对比: https://licyk.netlify.app/2025/02/10/debiased_estimation_loss_in_stable_diffusion_model_training\n",
    "# 启用后能提高拟合速度和颜色表现吧, 画风的学习能学得更好\n",
    "# 但, 肢体崩坏率可能会有点提高, 不过有另一套参数去优化了一下这个问题, 貌似会好一点\n",
    "# 可能画风会弱化, 所以不是很确定哪个比较好用, 只能自己试了\n",
    "# debiased estimation loss 有个相关的论文可以看看: https://arxiv.org/abs/2310.08442\n",
    "# \n",
    "# toml_file_path = os.path.join(WORKSPACE, \"train_config.toml\")\n",
    "# toml_content = f\"\"\"\n",
    "# pretrained_model_name_or_path = \"{SD_MODEL_PATH}/Illustrious-XL-v0.1.safetensors\"\n",
    "# vae = \"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\"\n",
    "# train_data_dir = \"{INPUT_DATASET_PATH}/Nachoneko\"\n",
    "# output_name = \"Nachoneko_2\"\n",
    "# output_dir = \"{OUTPUT_PATH}/Nachoneko\"\n",
    "# wandb_run_name = \"Nachoneko\"\n",
    "# log_tracker_name = \"lora-Nachoneko\"\n",
    "# prior_loss_weight = 1\n",
    "# resolution = \"1024,1024\"\n",
    "# enable_bucket = true\n",
    "# min_bucket_reso = 256\n",
    "# max_bucket_reso = 4096\n",
    "# bucket_reso_steps = 64\n",
    "# save_model_as = \"safetensors\"\n",
    "# save_precision = \"fp16\"\n",
    "# save_every_n_epochs = 1\n",
    "# max_train_epochs = 40\n",
    "# train_batch_size = 6\n",
    "# gradient_checkpointing = true\n",
    "# network_train_unet_only = true\n",
    "# learning_rate = 0.0001\n",
    "# unet_lr = 0.0001\n",
    "# text_encoder_lr = 0.00001\n",
    "# lr_scheduler = \"constant_with_warmup\"\n",
    "# lr_warmup_steps = 100\n",
    "# optimizer_type = \"Lion8bit\"\n",
    "# network_module = \"lycoris.kohya\"\n",
    "# network_dim = 100000\n",
    "# network_alpha = 100000\n",
    "# network_args = [\n",
    "#     \"conv_dim=100000\",\n",
    "#     \"conv_alpha=100000\",\n",
    "#     \"algo=lokr\",\n",
    "#     \"dropout=0\",\n",
    "#     \"factor=8\",\n",
    "#     \"train_norm=True\",\n",
    "#     \"preset=full\",\n",
    "# ]\n",
    "# optimizer_args = [\n",
    "#     \"weight_decay=0.05\",\n",
    "#     \"betas=0.9,0.95\",\n",
    "# ]\n",
    "# log_with = \"{LOG_MODULE}\"\n",
    "# logging_dir = \"{OUTPUT_PATH}/logs\"\n",
    "# caption_extension = \".txt\"\n",
    "# shuffle_caption = true\n",
    "# keep_tokens = 0\n",
    "# max_token_length = 225\n",
    "# seed = 1337\n",
    "# mixed_precision = \"fp16\"\n",
    "# xformers = true\n",
    "# cache_latents = true\n",
    "# cache_latents_to_disk = true\n",
    "# persistent_data_loader_workers = true\n",
    "# debiased_estimation_loss = true\n",
    "# vae_batch_size = 4\n",
    "# full_fp16 = true\n",
    "# \"\"\".strip()\n",
    "# if not os.path.exists(os.path.dirname(toml_file_path)):\n",
    "#     os.makedirs(toml_file_path, exist_ok=True)\n",
    "# with open(toml_file_path, \"w\", encoding=\"utf8\") as file:\n",
    "#     file.write(toml_content)\n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --config_file=\"{toml_file_path}\"\n",
    "\n",
    "\n",
    "# (自己在用的, toml 格式的版本)\n",
    "# 使用 lokr 算法训练 XL 画风 LoRA, 使用多卡进行训练\n",
    "# 该参数也可以用于人物 LoRA 训练\n",
    "# \n",
    "# 在训练多画风 LoRA 或者人物 LoRA 时, 通常会打上触发词\n",
    "# 当使用了 --network_train_unet_only 后, Text Encoder 虽然不会训练, 但并不影响将触发词训练进 LoRA 模型中\n",
    "# 并且不训练 Text Encoder 避免 Text Encoder 被炼烂(Text Encoder 比较容易被炼烂)\n",
    "# \n",
    "# 学习率调度器从 cosine_with_restarts 换成 constant_with_warmup, 此时学习率靠优化器(Lion8bit)进行调度\n",
    "# 拟合速度会更快\n",
    "# constant_with_warmup 用在大规模的训练上比较好, 但用在小规模训练也有不错的效果\n",
    "# 如果训练集的图比较少, 重复的图较多, 重复次数较高, 可能容易造成过拟合\n",
    "# \n",
    "# 在 --network_args 设置了 preset, 可以调整训练网络的大小\n",
    "# 该值默认为 full, 如果使用 attn-mlp 可以得到更小的 LoRA, 但对于难学的概念使用 full 效果会更好 (最好还是 full 吧, 其他的预设效果不是很好)\n",
    "# \n",
    "# 可用的预设可阅读文档: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Preset.md\n",
    "# 该预设也可以自行编写并指定, 编写例子可查看: https://github.com/KohakuBlueleaf/LyCORIS/blob/main/example_configs/preset_configs/example.toml\n",
    "# \n",
    "# 使用 --optimizer_args 设置 weight_decay 和 betas, 更高的 weight_decay 可以降低拟合程度, 减少过拟合\n",
    "# 如果拟合程度不够高, 可以提高 --max_train_epochs 的值, 或者适当降低 weight_decay 的值, 可自行测试\n",
    "# 较小的训练集适合使用较小的值, 如 0.05, 较大的训练集适合用 0.1\n",
    "# 大概 34 Epoch 会有比较好的效果吧, 不过不好说, 看训练集\n",
    "# 自己测的时候大概在 26~40 Epoch 之间会出现好结果, 测试了很多炉基本都在这个区间里, 但也不排除意外情况 (训练参数这东西好麻烦啊, 苦い)\n",
    "# \n",
    "# 测试的时候发现 --debiased_estimation_loss 对于训练效果的有些改善\n",
    "# 这里有个对比: https://licyk.netlify.app/2025/02/10/debiased_estimation_loss_in_stable_diffusion_model_training\n",
    "# 启用后能提高拟合速度和颜色表现吧, 画风的学习能学得更好\n",
    "# 但, 肢体崩坏率可能会有点提高, 不过有另一套参数去优化了一下这个问题, 貌似会好一点\n",
    "# 可能画风会弱化, 所以不是很确定哪个比较好用, 只能自己试了\n",
    "# debiased estimation loss 有个相关的论文可以看看: https://arxiv.org/abs/2310.08442\n",
    "# \n",
    "# 加上 v 预测参数进行训练, 提高模型对暗处和亮处的表现效果, 并且能让模型能够直出纯黑色背景, 画面也更干净\n",
    "# 相关的论文可以看看: https://arxiv.org/abs/2305.08891\n",
    "# \n",
    "# toml_file_path = os.path.join(WORKSPACE, \"train_config.toml\")\n",
    "# toml_content = f\"\"\"\n",
    "# pretrained_model_name_or_path = \"{SD_MODEL_PATH}/noobaiXLNAIXL_vPred10Version.safetensors\"\n",
    "# vae = \"{SD_MODEL_PATH}/sdxl_fp16_fix_vae.safetensors\"\n",
    "# train_data_dir = \"{INPUT_DATASET_PATH}/Nachoneko\"\n",
    "# output_name = \"Nachoneko_2\"\n",
    "# output_dir = \"{OUTPUT_PATH}/Nachoneko\"\n",
    "# wandb_run_name = \"Nachoneko\"\n",
    "# log_tracker_name = \"lora-Nachoneko\"\n",
    "# prior_loss_weight = 1\n",
    "# resolution = \"1024,1024\"\n",
    "# enable_bucket = true\n",
    "# min_bucket_reso = 256\n",
    "# max_bucket_reso = 4096\n",
    "# bucket_reso_steps = 64\n",
    "# save_model_as = \"safetensors\"\n",
    "# save_precision = \"fp16\"\n",
    "# save_every_n_epochs = 1\n",
    "# max_train_epochs = 40\n",
    "# train_batch_size = 6\n",
    "# gradient_checkpointing = true\n",
    "# network_train_unet_only = true\n",
    "# learning_rate = 0.0001\n",
    "# unet_lr = 0.0001\n",
    "# text_encoder_lr = 0.00001\n",
    "# lr_scheduler = \"constant_with_warmup\"\n",
    "# lr_warmup_steps = 100\n",
    "# optimizer_type = \"Lion8bit\"\n",
    "# network_module = \"lycoris.kohya\"\n",
    "# network_dim = 100000\n",
    "# network_alpha = 100000\n",
    "# network_args = [\n",
    "#     \"conv_dim=100000\",\n",
    "#     \"conv_alpha=100000\",\n",
    "#     \"algo=lokr\",\n",
    "#     \"dropout=0\",\n",
    "#     \"factor=8\",\n",
    "#     \"train_norm=True\",\n",
    "#     \"preset=full\",\n",
    "# ]\n",
    "# optimizer_args = [\n",
    "#     \"weight_decay=0.05\",\n",
    "#     \"betas=0.9,0.95\",\n",
    "# ]\n",
    "# log_with = \"{LOG_MODULE}\"\n",
    "# logging_dir = \"{OUTPUT_PATH}/logs\"\n",
    "# caption_extension = \".txt\"\n",
    "# shuffle_caption = true\n",
    "# keep_tokens = 0\n",
    "# max_token_length = 225\n",
    "# seed = 1337\n",
    "# mixed_precision = \"fp16\"\n",
    "# xformers = true\n",
    "# cache_latents = true\n",
    "# cache_latents_to_disk = true\n",
    "# persistent_data_loader_workers = true\n",
    "# debiased_estimation_loss = true\n",
    "# vae_batch_size = 4\n",
    "# zero_terminal_snr = true\n",
    "# v_parameterization = true\n",
    "# scale_v_pred_loss_like_noise_pred = true\n",
    "# full_fp16 = true\n",
    "# \"\"\".strip()\n",
    "# if not os.path.exists(os.path.dirname(toml_file_path)):\n",
    "#     os.makedirs(toml_file_path, exist_ok=True)\n",
    "# with open(toml_file_path, \"w\", encoding=\"utf8\") as file:\n",
    "#     file.write(toml_content)\n",
    "# !python -m accelerate.commands.launch \\\n",
    "#     --num_cpu_threads_per_process=1 \\\n",
    "#     --multi_gpu \\\n",
    "#     --num_processes=2 \\\n",
    "#     \"{SD_SCRIPTS_PATH}/sdxl_train_network.py\" \\\n",
    "#     --config_file=\"{toml_file_path}\"\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "os.chdir(WORKSPACE)\n",
    "logger.info(\"离开 sd-scripts 目录\")\n",
    "logger.info(\"模型训练结束\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型上传\n",
    "通常不需要修改该单元内容，如果需要修改参数，建议通过上方的参数配置单元进行修改  \n",
    "5. [← 上一个单元](#模型训练)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型上传到 HuggingFace / ModelScope, 通常不需要修改, 修改参数建议通过上方的参数配置单元进行修改\n",
    "\n",
    "# 使用 HuggingFace 上传模型\n",
    "if USE_HF_TO_SAVE_MODEL:\n",
    "    logger.info(\"使用 HuggingFace 保存模型\")\n",
    "    sd_scripts.repo.upload_files_to_repo(\n",
    "        api_type=\"huggingface\",         # 指定使用 HuggingFace 仓库\n",
    "        repo_id=HF_REPO_ID,             # HuggingFace 仓库地址\n",
    "        repo_type=HF_REPO_TYPE,         # HuggingFace 仓库种类\n",
    "        visibility=HF_REPO_VISIBILITY,  # 当尝试创建仓库时设置仓库的可见性\n",
    "        upload_path=OUTPUT_PATH,        # 要上传文件的目录\n",
    "        retry=RETRY,                    # 重试上传的次数\n",
    "    )\n",
    "\n",
    "# 使用 ModelScope 上传模型\n",
    "if USE_MS_TO_SAVE_MODEL:\n",
    "    logger.info(\"使用 ModelScope 保存模型\")\n",
    "    sd_scripts.repo.upload_files_to_repo(\n",
    "        api_type=\"modelscope\",          # 指定使用 ModelScope 仓库\n",
    "        repo_id=MS_REPO_ID,             # Modelscope 的仓库地址\n",
    "        repo_type=MS_REPO_TYPE,         # ModelScope 仓库的种类\n",
    "        visibility=MS_REPO_VISIBILITY,  # 当尝试创建仓库时设置仓库的可见性\n",
    "        upload_path=OUTPUT_PATH,        # 要上传文件的目录\n",
    "        retry=RETRY,                    # 重试上传的次数\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
